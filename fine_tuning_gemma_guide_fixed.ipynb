{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Gemma 3 1B Instruct: Complete Guide 🎯\n",
    "\n",
    "Welcome to the comprehensive guide for fine-tuning Google's Gemma 3 1B Instruct model! This notebook will walk you through the entire process of customizing a pre-trained language model for your specific use case.\n",
    "\n",
    "## What you'll learn:\n",
    "- Understanding fine-tuning vs training from scratch\n",
    "- Setting up the environment for different devices (CPU, CUDA, Apple Silicon)\n",
    "- Loading and preparing the Gemma 3 1B Instruct model\n",
    "- Creating and formatting training datasets\n",
    "- Implementing LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "- Training with different optimization techniques\n",
    "- Evaluating and testing your fine-tuned model\n",
    "- Saving and sharing your custom model\n",
    "\n",
    "## Prerequisites:\n",
    "- Python 3.8 or higher\n",
    "- At least 16GB of RAM (32GB+ recommended)\n",
    "- GPU with 8GB+ VRAM (or Apple Silicon with 16GB+ unified memory)\n",
    "- HuggingFace account and token for Gemma access\n",
    "- Basic understanding of machine learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Fine-Tuning\n",
    "\n",
    "Before we start, let's understand what fine-tuning means and why it's powerful:\n",
    "\n",
    "### 🧠 **What is Fine-Tuning?**\n",
    "Fine-tuning takes a pre-trained model and adapts it to your specific task or domain by training it on your custom dataset.\n",
    "\n",
    "### 🎯 **Types of Fine-Tuning:**\n",
    "- **Full Fine-Tuning**: Updates all model parameters (expensive, high quality)\n",
    "- **LoRA (Low-Rank Adaptation)**: Updates only small adapter layers (efficient, good quality)\n",
    "- **Prompt Tuning**: Learns optimal prompts (very efficient, task-specific)\n",
    "\n",
    "### 💡 **Why Fine-Tune Gemma 3 1B Instruct?**\n",
    "- Smaller model = faster training and inference\n",
    "- Good performance for many tasks\n",
    "- Fits in consumer hardware\n",
    "- Already instruction-tuned for better baseline\n",
    "\n",
    "### 📊 **Device Considerations:**\n",
    "- **Apple Silicon (M1/M2/M3)**: Great for LoRA fine-tuning, unified memory advantage\n",
    "- **NVIDIA GPUs**: Excellent for all types of fine-tuning\n",
    "- **CPU Only**: Possible but slow, best for very small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Required Libraries\n",
    "\n",
    "Let's install all the necessary libraries for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing fine-tuning packages...\n",
      "⚠️  This may take several minutes\n",
      "\n",
      "Installing transformers>=4.36.0...\n",
      "Requirement already satisfied: transformers>=4.36.0 in ./myenv/lib/python3.12/site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (2025.9.18)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->transformers>=4.36.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->transformers>=4.36.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->transformers>=4.36.0) (2025.8.3)\n",
      "✅ transformers>=4.36.0 installed successfully\n",
      "Installing torch>=2.1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.1.0 in ./myenv/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
      "✅ torch>=2.1.0 installed successfully\n",
      "Installing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./myenv/lib/python3.12/site-packages (4.1.1)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./myenv/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./myenv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./myenv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./myenv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./myenv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./myenv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in ./myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./myenv/lib/python3.12/site-packages (from datasets) (0.35.0)\n",
      "Requirement already satisfied: packaging in ./myenv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "✅ datasets installed successfully\n",
      "Installing accelerate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in ./myenv/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./myenv/lib/python3.12/site-packages (from accelerate) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./myenv/lib/python3.12/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in ./myenv/lib/python3.12/site-packages (from accelerate) (0.35.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./myenv/lib/python3.12/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
      "✅ accelerate installed successfully\n",
      "Installing peft...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in ./myenv/lib/python3.12/site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.12/site-packages (from peft) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (from peft) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.12/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./myenv/lib/python3.12/site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: transformers in ./myenv/lib/python3.12/site-packages (from peft) (4.56.1)\n",
      "Requirement already satisfied: tqdm in ./myenv/lib/python3.12/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./myenv/lib/python3.12/site-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in ./myenv/lib/python3.12/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in ./myenv/lib/python3.12/site-packages (from peft) (0.35.0)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2025.9.0)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.10)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.12/site-packages (from transformers->peft) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./myenv/lib/python3.12/site-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n",
      "✅ peft installed successfully\n",
      "Installing bitsandbytes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in ./myenv/lib/python3.12/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in ./myenv/lib/python3.12/site-packages (from bitsandbytes) (1.16.2)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in ./myenv/lib/python3.12/site-packages (from scipy->bitsandbytes) (2.3.3)\n",
      "✅ bitsandbytes installed successfully\n",
      "Installing trl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trl in ./myenv/lib/python3.12/site-packages (0.23.0)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in ./myenv/lib/python3.12/site-packages (from trl) (1.10.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in ./myenv/lib/python3.12/site-packages (from trl) (4.1.1)\n",
      "Requirement already satisfied: transformers>=4.56.1 in ./myenv/lib/python3.12/site-packages (from trl) (4.56.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (25.0)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (0.35.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in ./myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.12/site-packages (from transformers>=4.56.1->trl) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./myenv/lib/python3.12/site-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
      "✅ trl installed successfully\n",
      "Installing psutil...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (7.1.0)\n",
      "✅ psutil installed successfully\n",
      "Installing sentencepiece...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in ./myenv/lib/python3.12/site-packages (0.2.1)\n",
      "✅ sentencepiece installed successfully\n",
      "Installing protobuf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in ./myenv/lib/python3.12/site-packages (6.32.1)\n",
      "✅ protobuf installed successfully\n",
      "\n",
      "🎉 Installation complete!\n",
      "\n",
      "💡 Note: Some packages may show warnings - this is normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries for fine-tuning\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Core libraries for fine-tuning\n",
    "packages = [\n",
    "    \"transformers>=4.36.0\",    # Latest transformers with Gemma support\n",
    "    \"torch>=2.1.0\",           # PyTorch with MPS support\n",
    "    \"datasets\",               # For dataset handling\n",
    "    \"accelerate\",             # For distributed training\n",
    "    \"peft\",                   # For LoRA and other parameter-efficient methods\n",
    "    \"bitsandbytes\",           # For quantization (if supported)\n",
    "    \"trl\",                    # For training utilities\n",
    "    \"psutil\",                 # For system monitoring\n",
    "    \"sentencepiece\",          # For tokenization\n",
    "    \"protobuf\",               # Required for some tokenizers\n",
    "]\n",
    "\n",
    "print(\"📦 Installing fine-tuning packages...\")\n",
    "print(\"⚠️  This may take several minutes\")\n",
    "print()\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        install_package(package)\n",
    "        print(f\"✅ {package} installed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to install {package}: {e}\")\n",
    "        if \"bitsandbytes\" in package:\n",
    "            print(\"💡 bitsandbytes may not be available on Apple Silicon - this is OK\")\n",
    "\n",
    "print(\"\\n🎉 Installation complete!\")\n",
    "print(\"\\n💡 Note: Some packages may show warnings - this is normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Environment Setup and Device Detection\n",
    "\n",
    "Let's set up our environment and detect the best device for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alfredopetruolo/Desktop/Code/ollama_guide/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️  SYSTEM INFORMATION\n",
      "============================================================\n",
      "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n",
      "PyTorch version: 2.8.0\n",
      "Transformers version: 4.56.1\n",
      "\n",
      "💾 MEMORY INFORMATION:\n",
      "Total RAM: 24.0 GB\n",
      "Available RAM: 12.7 GB\n",
      "RAM usage: 47.1%\n",
      "\n",
      "🚀 DEVICE DETECTION:\n",
      "✅ Apple Silicon (MPS) available\n",
      "   Unified memory: 24.0 GB\n",
      "   MPS is ideal for LoRA fine-tuning\n",
      "\n",
      "🎯 Selected device: mps\n",
      "\n",
      "📋 TRAINING RECOMMENDATIONS:\n",
      "   • LoRA fine-tuning recommended\n",
      "   • Batch size: 2-8 depending on memory\n",
      "   • Use float16 precision\n",
      "\n",
      "✅ Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import transformers  # Import the module itself to access __version__\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import psutil\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# System information\n",
    "print(\"🖥️  SYSTEM INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Memory information\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"\\n💾 MEMORY INFORMATION:\")\n",
    "print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "print(f\"RAM usage: {memory.percent:.1f}%\")\n",
    "\n",
    "# Device detection with detailed information\n",
    "print(f\"\\n🚀 DEVICE DETECTION:\")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"✅ CUDA available with {gpu_count} GPU(s)\")\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        print(f\"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(f\"✅ Apple Silicon (MPS) available\")\n",
    "    print(f\"   Unified memory: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"   MPS is ideal for LoRA fine-tuning\")\n",
    "    \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"⚠️  Using CPU only\")\n",
    "    print(f\"   Training will be slower but still possible\")\n",
    "    print(f\"   Consider using smaller batch sizes\")\n",
    "\n",
    "print(f\"\\n🎯 Selected device: {device}\")\n",
    "\n",
    "# Training recommendations based on device\n",
    "print(f\"\\n📋 TRAINING RECOMMENDATIONS:\")\n",
    "if device == \"cuda\":\n",
    "    print(\"   • Use LoRA or full fine-tuning\")\n",
    "    print(\"   • Batch size: 4-16 depending on GPU memory\")\n",
    "    print(\"   • Enable gradient checkpointing for larger models\")\n",
    "elif device == \"mps\":\n",
    "    print(\"   • LoRA fine-tuning recommended\")\n",
    "    print(\"   • Batch size: 2-8 depending on memory\")\n",
    "    print(\"   • Use float16 precision\")\n",
    "else:\n",
    "    print(\"   • LoRA fine-tuning only\")\n",
    "    print(\"   • Small batch size: 1-2\")\n",
    "    print(\"   • Consider using smaller dataset\")\n",
    "\n",
    "print(\"\\n✅ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: HuggingFace Authentication\n",
    "\n",
    "Gemma models require authentication. Let's set up your HuggingFace token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 HUGGINGFACE AUTHENTICATION\n",
      "==================================================\n",
      "✅ Already authenticated as: bobbinetor\n",
      "   Email: petruolo95@gmail.com\n",
      "\n",
      "🎉 Ready to proceed with Gemma model loading!\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Authentication Setup\n",
    "print(\"🔐 HUGGINGFACE AUTHENTICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if user is already logged in\n",
    "from huggingface_hub import HfApi\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"✅ Already authenticated as: {user_info['name']}\")\n",
    "    print(f\"   Email: {user_info.get('email', 'Not provided')}\")\n",
    "    HF_TOKEN = True\n",
    "except Exception:\n",
    "    print(\"❌ Not authenticated with HuggingFace\")\n",
    "    HF_TOKEN = False\n",
    "\n",
    "# If not authenticated, provide instructions\n",
    "if not HF_TOKEN:\n",
    "    print(\"\\n🔑 TO ACCESS GEMMA MODELS:\")\n",
    "    print(\"1. Go to https://huggingface.co/settings/tokens\")\n",
    "    print(\"2. Create a new token with 'Read' permissions\")\n",
    "    print(\"3. Accept the Gemma license at: https://huggingface.co/google/gemma-3-1b-it\")\n",
    "    print(\"4. Run: huggingface-cli login\")\n",
    "    print(\"5. Paste your token when prompted\")\n",
    "    print(\"\\n💡 Alternative: Set HF_TOKEN environment variable\")\n",
    "    print(\"   export HF_TOKEN=your_token_here\")\n",
    "    \n",
    "    # Check for environment variable\n",
    "    import os\n",
    "    if 'HF_TOKEN' in os.environ:\n",
    "        print(\"\\n✅ Found HF_TOKEN in environment variables\")\n",
    "        HF_TOKEN = True\n",
    "    else:\n",
    "        print(\"\\n⚠️  No HF_TOKEN found. Please authenticate before proceeding.\")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    print(\"\\n🎉 Ready to proceed with Gemma model loading!\")\n",
    "else:\n",
    "    print(\"\\n⏹️  Please complete authentication before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load and Prepare the Gemma 3 1B Instruct Model\n",
    "\n",
    "Now let's load the Gemma 3 1B Instruct model with proper configuration for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 LOADING GEMMA 3 1B INSTRUCT MODEL\n",
      "==================================================\n",
      "Model: google/gemma-3-1b-it\n",
      "Device: mps\n",
      "📥 Loading tokenizer...\n",
      "✅ Tokenizer loaded successfully\n",
      "   Vocabulary size: 262145\n",
      "   Pad token: <pad>\n",
      "   EOS token: <eos>\n",
      "\n",
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully\n",
      "\n",
      "📊 MODEL INFORMATION:\n",
      "   Parameters: 999,885,952\n",
      "   Model size: ~2.0 GB (FP16)\n",
      "   Device: mps\n",
      "   Data type: torch.float16\n",
      "\n",
      "🧪 TOKENIZER TEST:\n",
      "   Test text: '### Instruction:\\nHello\\n\\n### Response:\\n'\n",
      "   Tokens: 12\n",
      "   Decoded back: '<bos>### Instruction:\\nHello\\n\\n### Response:\\n<eos>'\n",
      "\n",
      "✅ Model setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare Gemma 3 1B Instruct model\n",
    "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "\n",
    "print(f\"🤖 LOADING GEMMA 3 1B INSTRUCT MODEL\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Memory optimization settings\n",
    "torch_dtype = torch.float16 if device != \"cpu\" else torch.float32\n",
    "\n",
    "try:\n",
    "    print(\"📥 Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        add_eos_token=True,\n",
    "    )\n",
    "    \n",
    "    # Set pad token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(\"✅ Tokenizer loaded successfully\")\n",
    "    print(f\"   Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"   Pad token: {tokenizer.pad_token}\")\n",
    "    print(f\"   EOS token: {tokenizer.eos_token}\")\n",
    "    \n",
    "    print(\"\\n📥 Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch_dtype,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    )\n",
    "    \n",
    "    # Move model to device if not using device_map\n",
    "    if device != \"cuda\":\n",
    "        model = model.to(device)\n",
    "    \n",
    "    print(\"✅ Model loaded successfully\")\n",
    "    \n",
    "    # Model information\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n📊 MODEL INFORMATION:\")\n",
    "    print(f\"   Parameters: {num_params:,}\")\n",
    "    print(f\"   Model size: ~{num_params * 2 / 1e9:.1f} GB (FP16)\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Data type: {torch_dtype}\")\n",
    "    \n",
    "    # Memory usage check\n",
    "    if device == \"cuda\":\n",
    "        memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"   GPU memory used: {memory_used:.1f}GB / {memory_total:.1f}GB\")\n",
    "    \n",
    "    # Test tokenizer with a simple example\n",
    "    print(f\"\\n🧪 TOKENIZER TEST:\")\n",
    "    test_text = \"### Instruction:\\nHello\\n\\n### Response:\\n\"\n",
    "    test_tokens = tokenizer.encode(test_text)\n",
    "    print(f\"   Test text: {repr(test_text)}\")\n",
    "    print(f\"   Tokens: {len(test_tokens)}\")\n",
    "    print(f\"   Decoded back: {repr(tokenizer.decode(test_tokens))}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"💡 This might be due to:\")\n",
    "    print(\"   • HuggingFace authentication issues\")\n",
    "    print(\"   • Insufficient memory\")\n",
    "    print(\"   • Network connectivity\")\n",
    "    print(\"   • Missing model access permissions\")\n",
    "\n",
    "print(f\"\\n✅ Model setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setup LoRA Configuration (Optional: Customize Precision)\n",
    "\n",
    "We'll use LoRA (Low-Rank Adaptation) for efficient fine-tuning. You can also customize device and precision settings here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 SETTING UP LORA CONFIGURATION\n",
      "==================================================\n",
      "⚙️ DEVICE AND PRECISION SELECTION:\n",
      "Current auto-detected device: mps\n",
      "🔄 Precision overridden to: fp32\n",
      "✅ Selected configuration:\n",
      "   Device: mps\n",
      "   Precision: FP32\n",
      "\n",
      "🎯 LORA PARAMETERS:\n",
      "   Rank (r): 16\n",
      "   Alpha: 32\n",
      "   Dropout: 0.1\n",
      "   Target modules: 7 layers\n",
      "\n",
      "🔄 Applying LoRA to model...\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "✅ LoRA applied successfully!\n",
      "trainable params: 13,045,760 || all params: 1,012,931,712 || trainable%: 1.2879\n",
      "\n",
      "📊 PARAMETER EFFICIENCY:\n",
      "   Total parameters: 1,012,931,712\n",
      "   Trainable parameters: 13,045,760\n",
      "   Percentage trainable: 1.29%\n",
      "   Memory reduction: ~98.7%\n",
      "\n",
      "✅ LoRA setup complete! Ready for efficient fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "# Setup LoRA Configuration and Device/Precision Selection\n",
    "print(\"🔧 SETTING UP LORA CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if model exists\n",
    "if 'model' not in globals():\n",
    "    print(\"❌ Error: 'model' variable not found!\")\n",
    "    print(\"💡 Please run Step 5 (Load Gemma 3 1B model) first\")\n",
    "else:\n",
    "    # === DEVICE AND PRECISION CUSTOMIZATION ===\n",
    "    print(\"⚙️ DEVICE AND PRECISION SELECTION:\")\n",
    "    print(\"Current auto-detected device:\", device)\n",
    "    \n",
    "    # Allow user to override device selection\n",
    "    FORCE_DEVICE = None  # Set to \"cuda\", \"mps\", or \"cpu\" to override auto-detection\n",
    "    FORCE_PRECISION = \"fp32\"  # Set to \"fp16\", \"bf16\", or \"fp32\" to override auto-selection\n",
    "    \n",
    "    # Apply device override if specified\n",
    "    if FORCE_DEVICE:\n",
    "        device = FORCE_DEVICE\n",
    "        print(f\"🔄 Device overridden to: {device}\")\n",
    "    \n",
    "    # Determine precision settings\n",
    "    if FORCE_PRECISION:\n",
    "        if FORCE_PRECISION == \"fp16\":\n",
    "            use_fp16, use_bf16 = True, False\n",
    "        elif FORCE_PRECISION == \"bf16\":\n",
    "            use_fp16, use_bf16 = False, True\n",
    "        else:  # fp32\n",
    "            use_fp16, use_bf16 = False, False\n",
    "        print(f\"🔄 Precision overridden to: {FORCE_PRECISION}\")\n",
    "    else:\n",
    "        # Auto-select precision based on device\n",
    "        if device == \"cuda\":\n",
    "            use_fp16, use_bf16 = True, False  # FP16 for CUDA\n",
    "        elif device == \"mps\":\n",
    "            use_fp16, use_bf16 = False, False  # FP32 for MPS (compatibility)\n",
    "        else:\n",
    "            use_fp16, use_bf16 = False, False  # FP32 for CPU\n",
    "    \n",
    "    print(f\"✅ Selected configuration:\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Precision: {'FP16' if use_fp16 else 'BF16' if use_bf16 else 'FP32'}\")\n",
    "    \n",
    "    # === LORA CONFIGURATION ===\n",
    "    print(f\"\\n🎯 LORA PARAMETERS:\")\n",
    "    \n",
    "    # LoRA parameters (customizable)\n",
    "    LORA_R = 16        # Rank of adaptation (higher = more parameters, better quality)\n",
    "    LORA_ALPHA = 32    # LoRA scaling parameter (typically 2x rank)\n",
    "    LORA_DROPOUT = 0.1 # LoRA dropout (0.0-0.3)\n",
    "\n",
    "    # Define target modules for LoRA (Gemma-specific)\n",
    "    target_modules = [\n",
    "        \"q_proj\",     # Query projection\n",
    "        \"k_proj\",     # Key projection\n",
    "        \"v_proj\",     # Value projection\n",
    "        \"o_proj\",     # Output projection\n",
    "        \"gate_proj\",  # Gate projection (MLP)\n",
    "        \"up_proj\",    # Up projection (MLP)\n",
    "        \"down_proj\"   # Down projection (MLP)\n",
    "    ]\n",
    "\n",
    "    # Create LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "    print(f\"   Rank (r): {LORA_R}\")\n",
    "    print(f\"   Alpha: {LORA_ALPHA}\")\n",
    "    print(f\"   Dropout: {LORA_DROPOUT}\")\n",
    "    print(f\"   Target modules: {len(target_modules)} layers\")\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    print(f\"\\n🔄 Applying LoRA to model...\")\n",
    "    try:\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        print(\"✅ LoRA applied successfully!\")\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "        # Calculate parameter efficiency\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\n📊 PARAMETER EFFICIENCY:\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "        print(f\"   Memory reduction: ~{(total_params - trainable_params) / total_params * 100:.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error applying LoRA: {e}\")\n",
    "        print(\"💡 This might be due to model architecture or memory issues\")\n",
    "\n",
    "    print(f\"\\n✅ LoRA setup complete! Ready for efficient fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Prepare Training Dataset\n",
    "\n",
    "Let's create a sample dataset for fine-tuning. You can replace this with your own data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 PREPARING TRAINING DATASET\n",
      "==================================================\n",
      "📝 Created dataset with 100 examples\n",
      "\n",
      "📋 Example formatted training sample:\n",
      "----------------------------------------\n",
      "### Instruction:\n",
      "Explain what machine learning is in simple terms.\n",
      "\n",
      "### Response:\n",
      "Machine learning is a type of artificial intelligence where computers learn to make predictions or decisions by finding patterns in data, rather than being explicitly programmed for every task.\n",
      "----------------------------------------\n",
      "\n",
      "✅ Dataset created with 100 examples\n",
      "   Example keys: ['text']\n",
      "   Training examples: 80\n",
      "   Validation examples: 20\n",
      "\n",
      "💡 Note: In practice, you should use a much larger dataset (1000+ examples)\n",
      "   for better fine-tuning results. This is just a demonstration.\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataset for demonstration\n",
    "# Replace this with your own dataset!\n",
    "\n",
    "print(\"📚 PREPARING TRAINING DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample data: AI assistant training examples\n",
    "sample_data = [\n",
    "    {\n",
    "        \"instruction\": \"Explain what machine learning is in simple terms.\",\n",
    "        \"response\": \"Machine learning is a type of artificial intelligence where computers learn to make predictions or decisions by finding patterns in data, rather than being explicitly programmed for every task.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the benefits of renewable energy?\",\n",
    "        \"response\": \"Renewable energy sources like solar and wind power are sustainable, reduce greenhouse gas emissions, decrease dependence on fossil fuels, and can lead to long-term cost savings while creating jobs in green technology sectors.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How do you make a simple vegetable soup?\",\n",
    "        \"response\": \"To make vegetable soup: 1) Heat oil in a pot, 2) Sauté diced onions and garlic, 3) Add chopped vegetables like carrots, celery, and potatoes, 4) Pour in vegetable broth, 5) Season with herbs and spices, 6) Simmer until vegetables are tender, about 20-30 minutes.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the importance of exercise for mental health?\",\n",
    "        \"response\": \"Exercise significantly benefits mental health by releasing endorphins that improve mood, reducing stress and anxiety, improving sleep quality, boosting self-esteem, and providing a healthy outlet for managing emotions and daily pressures.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of photosynthesis.\",\n",
    "        \"response\": \"Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose (sugar) and oxygen. This occurs mainly in plant leaves using chlorophyll, providing energy for the plant and producing oxygen as a byproduct that's essential for most life on Earth.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extend the dataset by repeating and varying the examples\n",
    "# In practice, you'd want hundreds or thousands of examples\n",
    "extended_data = []\n",
    "for i in range(20):  # Repeat each example multiple times\n",
    "    for item in sample_data:\n",
    "        extended_data.append(item)\n",
    "\n",
    "print(f\"📝 Created dataset with {len(extended_data)} examples\")\n",
    "\n",
    "# Format data for instruction following\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format the data as instruction-following examples\"\"\"\n",
    "    return f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n",
    "\n",
    "# Apply formatting\n",
    "formatted_texts = [format_instruction(item) for item in extended_data]\n",
    "\n",
    "print(\"\\n📋 Example formatted training sample:\")\n",
    "print(\"-\" * 40)\n",
    "print(formatted_texts[0])\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "dataset = Dataset.from_dict({\"text\": formatted_texts})\n",
    "\n",
    "print(f\"\\n✅ Dataset created with {len(dataset)} examples\")\n",
    "print(f\"   Example keys: {list(dataset.features.keys())}\")\n",
    "\n",
    "# Split into train/validation\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "print(f\"   Training examples: {len(train_dataset)}\")\n",
    "print(f\"   Validation examples: {len(eval_dataset)}\")\n",
    "\n",
    "print(\"\\n💡 Note: In practice, you should use a much larger dataset (1000+ examples)\")\n",
    "print(\"   for better fine-tuning results. This is just a demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Tokenize the Dataset\n",
    "\n",
    "Now let's tokenize our dataset for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 TOKENIZING DATASET\n",
      "========================================\n",
      "✅ All required variables found!\n",
      "Max sequence length: 512\n",
      "🔄 Tokenizing training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing training data: 100%|██████████| 80/80 [00:00<00:00, 6041.92 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Tokenizing validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing validation data: 100%|██████████| 20/20 [00:00<00:00, 5250.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenization complete!\n",
      "\n",
      "📊 TOKENIZATION STATISTICS:\n",
      "   Sample input_ids length: 512\n",
      "   Sample attention_mask length: 512\n",
      "   Number of padding tokens in sample: 441\n",
      "\n",
      "🔍 SAMPLE TOKENIZED TEXT:\n",
      "First 50 tokens decoded: ...\n",
      "\n",
      "💾 Dataset ready for training!\n",
      "   Training samples: 80\n",
      "   Validation samples: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization configuration\n",
    "MAX_LENGTH = 512  # Adjust based on your data and memory\n",
    "\n",
    "print(f\"🔤 TOKENIZING DATASET\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check if required variables exist\n",
    "missing_vars = []\n",
    "if 'tokenizer' not in globals():\n",
    "    missing_vars.append('tokenizer (from Step 5)')\n",
    "if 'train_dataset' not in globals():\n",
    "    missing_vars.append('train_dataset (from Step 6)')\n",
    "if 'eval_dataset' not in globals():\n",
    "    missing_vars.append('eval_dataset (from Step 6)')\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"❌ ERROR: Missing required variables!\")\n",
    "    print(\"💡 The following variables are not defined:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"   • {var}\")\n",
    "    print(\"\\n🔄 REQUIRED STEPS:\")\n",
    "    print(\"   1. Run Step 4: HuggingFace authentication\")\n",
    "    print(\"   2. Run Step 5: Load Gemma 3 1B model (creates tokenizer)\")\n",
    "    print(\"   3. Run Step 6: Prepare training dataset (creates train_dataset, eval_dataset)\")\n",
    "    print(\"   4. Then run this Step 7: Tokenize the dataset\")\n",
    "    \n",
    "    # Show what variables ARE defined\n",
    "    defined_vars = [var for var in globals().keys() if not var.startswith('_') and var not in ['In', 'Out', 'get_ipython']]\n",
    "    print(f\"\\n📋 Currently defined variables: {', '.join(sorted(defined_vars))}\")\n",
    "    \n",
    "    # Exit early to prevent further errors\n",
    "    print(\"\\n⏹️ Stopping execution. Please run the missing steps first.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"✅ All required variables found!\")\n",
    "    print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        \"\"\"Tokenize the text examples\"\"\"\n",
    "        # Tokenize the texts\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        # For causal LM, labels are the same as input_ids\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        \n",
    "        return tokenized\n",
    "\n",
    "    # Tokenize datasets\n",
    "    print(\"🔄 Tokenizing training dataset...\")\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        desc=\"Tokenizing training data\"\n",
    "    )\n",
    "\n",
    "    print(\"🔄 Tokenizing validation dataset...\")\n",
    "    tokenized_eval = eval_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=eval_dataset.column_names,\n",
    "        desc=\"Tokenizing validation data\"\n",
    "    )\n",
    "\n",
    "    print(\"✅ Tokenization complete!\")\n",
    "\n",
    "    # Examine tokenized data\n",
    "    sample_tokens = tokenized_train[0]\n",
    "    print(f\"\\n📊 TOKENIZATION STATISTICS:\")\n",
    "    print(f\"   Sample input_ids length: {len(sample_tokens['input_ids'])}\")\n",
    "    print(f\"   Sample attention_mask length: {len(sample_tokens['attention_mask'])}\")\n",
    "    print(f\"   Number of padding tokens in sample: {sample_tokens['attention_mask'].count(0)}\")\n",
    "\n",
    "    # Show a sample of tokenized text\n",
    "    print(f\"\\n🔍 SAMPLE TOKENIZED TEXT:\")\n",
    "    sample_text = tokenizer.decode(sample_tokens['input_ids'][:50], skip_special_tokens=True)\n",
    "    print(f\"First 50 tokens decoded: {sample_text}...\")\n",
    "\n",
    "    print(f\"\\n💾 Dataset ready for training!\")\n",
    "    print(f\"   Training samples: {len(tokenized_train)}\")\n",
    "    print(f\"   Validation samples: {len(tokenized_eval)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Configure Training Arguments\n",
    "\n",
    "Let's set up training parameters optimized for different devices and precision settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  CONFIGURING TRAINING ARGUMENTS\n",
      "==================================================\n",
      "✅ All required variables found!\n",
      "   Optimizing for device: mps\n",
      "   Using precision: FP32\n",
      "\n",
      "🎯 TRAINING PARAMETERS:\n",
      "   Batch size: 2\n",
      "   Gradient accumulation steps: 4\n",
      "   Effective batch size: 8\n",
      "   Epochs: 3\n",
      "   Learning rate: 0.0002\n",
      "   Weight decay: 0.01\n",
      "\n",
      "📋 TRAINING CONFIGURATION SUMMARY:\n",
      "   Device: mps\n",
      "   Precision: FP32\n",
      "   Epochs: 3\n",
      "   Learning rate: 0.0002\n",
      "   Effective batch size: 8\n",
      "   Gradient checkpointing: True\n",
      "\n",
      "⏱️  TRAINING ESTIMATES:\n",
      "   Total training steps: 30\n",
      "   Estimated time on Apple Silicon (FP32): ~22.5 minutes\n",
      "\n",
      "✅ Training arguments configured!\n",
      "\n",
      "📋 NEXT STEPS:\n",
      "   • Step 10: Setup trainer\n",
      "   • Step 11: Start training\n"
     ]
    }
   ],
   "source": [
    "# Training configuration based on device and precision settings\n",
    "print(f\"⚙️  CONFIGURING TRAINING ARGUMENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if required variables exist\n",
    "missing_vars = []\n",
    "if 'device' not in globals():\n",
    "    missing_vars.append('device (from Step 3)')\n",
    "if 'tokenized_train' not in globals():\n",
    "    missing_vars.append('tokenized_train (from Step 8)')\n",
    "if 'use_fp16' not in globals():\n",
    "    missing_vars.append('use_fp16 (from Step 6)')\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"❌ ERROR: Missing required variables!\")\n",
    "    print(\"💡 The following variables are not defined:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"   • {var}\")\n",
    "    print(\"\\n🔄 REQUIRED STEPS:\")\n",
    "    print(\"   1. Run Step 3: Environment Setup (creates device)\")\n",
    "    print(\"   2. Run Steps 4-8: Complete model and dataset preparation\")\n",
    "    print(\"   3. Then run this Step 9: Configure Training Arguments\")\n",
    "    \n",
    "    # Use fallback values\n",
    "    device = \"mps\"  # Default fallback\n",
    "    use_fp16, use_bf16 = False, False  # Safe defaults\n",
    "    print(f\"\\n🔄 Using fallback values:\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Precision: FP32 (safe default)\")\n",
    "    print(\"⚠️  Training time estimates will be inaccurate without tokenized_train\")\n",
    "else:\n",
    "    print(f\"✅ All required variables found!\")\n",
    "\n",
    "print(f\"   Optimizing for device: {device}\")\n",
    "print(f\"   Using precision: {'FP16' if use_fp16 else 'BF16' if use_bf16 else 'FP32'}\")\n",
    "\n",
    "# === TRAINING PARAMETERS CONFIGURATION ===\n",
    "print(f\"\\n🎯 TRAINING PARAMETERS:\")\n",
    "\n",
    "# Device-specific training parameters\n",
    "if device == \"cuda\":\n",
    "    # CUDA optimized settings\n",
    "    batch_size = 4\n",
    "    gradient_accumulation_steps = 2\n",
    "    dataloader_num_workers = 4\n",
    "    \n",
    "elif device == \"mps\":\n",
    "    # Apple Silicon optimized settings\n",
    "    batch_size = 2\n",
    "    gradient_accumulation_steps = 4\n",
    "    dataloader_num_workers = 2\n",
    "    \n",
    "else:\n",
    "    # CPU settings\n",
    "    batch_size = 1\n",
    "    gradient_accumulation_steps = 8\n",
    "    dataloader_num_workers = 1\n",
    "\n",
    "# Effective batch size calculation\n",
    "effective_batch_size = batch_size * gradient_accumulation_steps\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {effective_batch_size}\")\n",
    "\n",
    "# Additional training parameters (customizable)\n",
    "NUM_EPOCHS = 3          # Number of training epochs\n",
    "LEARNING_RATE = 2e-4    # Learning rate\n",
    "WEIGHT_DECAY = 0.01     # Weight decay for regularization\n",
    "WARMUP_RATIO = 0.1      # Warmup ratio\n",
    "MAX_GRAD_NORM = 1.0     # Gradient clipping\n",
    "\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Weight decay: {WEIGHT_DECAY}\")\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"./gemma-3-1b-it-finetuned\"\n",
    "\n",
    "# Training arguments (updated for newer transformers versions)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    \n",
    "    # Precision - Uses values from Step 6\n",
    "    fp16=use_fp16,\n",
    "    bf16=use_bf16,\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=dataloader_num_workers,\n",
    "    \n",
    "    # Evaluation and logging (updated parameter names)\n",
    "    eval_strategy=\"steps\",          # Updated from evaluation_strategy\n",
    "    eval_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",          # Added explicit save strategy\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=[],  # Disable wandb for now\n",
    "    run_name=\"gemma-3-1b-it-finetune\",\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 TRAINING CONFIGURATION SUMMARY:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Precision: {'FP16' if use_fp16 else 'BF16' if use_bf16 else 'FP32'}\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Effective batch size: {effective_batch_size}\")\n",
    "print(f\"   Gradient checkpointing: {training_args.gradient_checkpointing}\")\n",
    "\n",
    "# Estimate training time (only if tokenized_train exists)\n",
    "print(f\"\\n⏱️  TRAINING ESTIMATES:\")\n",
    "if 'tokenized_train' in globals():\n",
    "    total_steps = len(tokenized_train) // effective_batch_size * training_args.num_train_epochs\n",
    "    print(f\"   Total training steps: {total_steps}\")\n",
    "    \n",
    "    # Adjust time estimates based on precision\n",
    "    if device == \"mps\":\n",
    "        base_time = 45 if not use_fp16 else 30  # FP32 vs FP16\n",
    "        estimated_time = total_steps * base_time / 60\n",
    "        precision_note = \"FP32\" if not use_fp16 else \"FP16\"\n",
    "        print(f\"   Estimated time on Apple Silicon ({precision_note}): ~{estimated_time:.1f} minutes\")\n",
    "    elif device == \"cuda\":\n",
    "        base_time = 10 if use_fp16 else 15  # FP16 vs FP32\n",
    "        estimated_time = total_steps * base_time / 60\n",
    "        precision_note = \"FP16\" if use_fp16 else \"BF16\" if use_bf16 else \"FP32\"\n",
    "        print(f\"   Estimated time on GPU ({precision_note}): ~{estimated_time:.1f} minutes\")\n",
    "    else:\n",
    "        estimated_time = total_steps * 120 / 60  # FP32 on CPU\n",
    "        print(f\"   Estimated time on CPU (FP32): ~{estimated_time:.1f} minutes\")\n",
    "else:\n",
    "    print(\"   ⚠️  Cannot estimate training time: tokenized_train not found\")\n",
    "    print(\"   💡 Complete Steps 4-8 first to get accurate estimates\")\n",
    "    print(\"   📊 Estimated steps: ~30 (assuming 100 examples, 3 epochs)\")\n",
    "\n",
    "print(f\"\\n✅ Training arguments configured!\")\n",
    "print(f\"\\n📋 NEXT STEPS:\")\n",
    "print(f\"   • Step 10: Setup trainer\")\n",
    "print(f\"   • Step 11: Start training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Setup Data Collator and Trainer\n",
    "\n",
    "Now let's set up the data collator and trainer with all our configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 SETTING UP DATA COLLATOR AND TRAINER\n",
      "==================================================\n",
      "✅ Data collator created\n",
      "🏋️ Creating trainer...\n",
      "✅ Trainer created successfully!\n",
      "\n",
      "📊 TRAINING SETUP SUMMARY:\n",
      "   Model: google/gemma-3-1b-it\n",
      "   Device: mps\n",
      "   Training method: LoRA fine-tuning\n",
      "   Training samples: 80\n",
      "   Validation samples: 20\n",
      "   Output directory: ./gemma-3-1b-it-finetuned\n",
      "   Mixed precision: FP16=False, BF16=False\n",
      "   Apple Silicon unified memory in use\n",
      "   System RAM: 62.1% used\n",
      "\n",
      "🎯 Ready to start training!\n"
     ]
    }
   ],
   "source": [
    "# Setup data collator and trainer\n",
    "print(\"📦 SETTING UP DATA COLLATOR AND TRAINER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if required variables exist\n",
    "missing_vars = []\n",
    "required_vars = ['tokenizer', 'model', 'training_args', 'tokenized_train', 'tokenized_eval']\n",
    "for var in required_vars:\n",
    "    if var not in globals():\n",
    "        missing_vars.append(var)\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"❌ ERROR: Missing required variables!\")\n",
    "    print(\"💡 The following variables are not defined:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"   • {var}\")\n",
    "    print(\"\\n🔄 REQUIRED STEPS:\")\n",
    "    print(\"   1. Run Steps 3-9 in order to create all required variables\")\n",
    "    print(\"   2. Then run this Step 10: Setup trainer\")\n",
    "    \n",
    "else:\n",
    "    # Data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # We're doing causal LM, not masked LM\n",
    "        pad_to_multiple_of=8,  # For efficiency\n",
    "    )\n",
    "\n",
    "    print(\"✅ Data collator created\")\n",
    "\n",
    "    # Create trainer\n",
    "    print(\"🏋️ Creating trainer...\")\n",
    "\n",
    "    try:\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_eval,\n",
    "            data_collator=data_collator,\n",
    "            processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Trainer created successfully!\")\n",
    "        \n",
    "        # Print training setup summary\n",
    "        print(f\"\\n📊 TRAINING SETUP SUMMARY:\")\n",
    "        print(f\"   Model: {MODEL_NAME}\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(f\"   Training method: LoRA fine-tuning\")\n",
    "        print(f\"   Training samples: {len(tokenized_train)}\")\n",
    "        print(f\"   Validation samples: {len(tokenized_eval)}\")\n",
    "        print(f\"   Output directory: {output_dir}\")\n",
    "        print(f\"   Mixed precision: FP16={training_args.fp16}, BF16={training_args.bf16}\")\n",
    "        \n",
    "        # Memory check before training\n",
    "        if device == \"cuda\":\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "            memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            print(f\"   GPU memory: {memory_used:.1f}GB / {memory_total:.1f}GB\")\n",
    "        elif device == \"mps\":\n",
    "            print(f\"   Apple Silicon unified memory in use\")\n",
    "        \n",
    "        current_memory = psutil.virtual_memory()\n",
    "        print(f\"   System RAM: {current_memory.percent:.1f}% used\")\n",
    "        \n",
    "        print(f\"\\n🎯 Ready to start training!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating trainer: {e}\")\n",
    "        print(\"💡 This might be due to:\")\n",
    "        print(\"   • Mixed precision compatibility issues\")\n",
    "        print(\"   • Memory or configuration issues\")\n",
    "        print(\"   • Missing variables from previous steps\")\n",
    "        \n",
    "        # Provide specific help for common issues\n",
    "        if \"fp16\" in str(e).lower() and device == \"mps\":\n",
    "            print(\"\\n🔧 SOLUTION: Re-run Step 6 and set FORCE_PRECISION='fp32'\")\n",
    "        elif \"tokenizer\" in str(e).lower():\n",
    "            print(\"\\n🔧 SOLUTION: Make sure you've run Step 5 (Load Model)\")\n",
    "        elif \"processing_class\" in str(e).lower():\n",
    "            print(\"\\n💡 Note: Using fallback tokenizer parameter for compatibility\")\n",
    "            # Fallback to older API\n",
    "            try:\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=tokenized_train,\n",
    "                    eval_dataset=tokenized_eval,\n",
    "                    data_collator=data_collator,\n",
    "                    tokenizer=tokenizer,  # Fallback to tokenizer parameter\n",
    "                )\n",
    "                print(\"✅ Trainer created with fallback method!\")\n",
    "            except Exception as fallback_e:\n",
    "                print(f\"❌ Fallback also failed: {fallback_e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Start Fine-Tuning\n",
    "\n",
    "Now let's start the actual fine-tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STARTING FINE-TUNING PROCESS\n",
      "==================================================\n",
      "⚠️  This may take some time depending on your hardware\n",
      "💡 Monitor GPU/CPU usage and temperature\n",
      "\n",
      "🏋️ Beginning training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Training failed: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "💡 Common issues:\n",
      "   • Out of memory: Reduce batch size or use gradient checkpointing\n",
      "   • Model too large: Use smaller model or more aggressive LoRA settings\n",
      "   • Device issues: Check CUDA/MPS availability\n",
      "📁 Partial model saved to: ./gemma-3-1b-it-finetuned_partial\n",
      "\n",
      "🏁 Training session complete!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"🚀 STARTING FINE-TUNING PROCESS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"⚠️  This may take some time depending on your hardware\")\n",
    "print(\"💡 Monitor GPU/CPU usage and temperature\")\n",
    "print()\n",
    "\n",
    "# Record training start time\n",
    "training_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    print(\"🏋️ Beginning training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_end_time = time.time()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    \n",
    "    print(f\"\\n🎉 TRAINING COMPLETED!\")\n",
    "    print(f\"   Total training time: {training_duration / 60:.1f} minutes\")\n",
    "    print(f\"   Average time per epoch: {training_duration / training_args.num_train_epochs / 60:.1f} minutes\")\n",
    "    \n",
    "    # Get training metrics\n",
    "    train_metrics = trainer.state.log_history\n",
    "    if train_metrics:\n",
    "        final_train_loss = None\n",
    "        final_eval_loss = None\n",
    "        \n",
    "        # Find the final losses\n",
    "        for log in reversed(train_metrics):\n",
    "            if 'train_loss' in log and final_train_loss is None:\n",
    "                final_train_loss = log['train_loss']\n",
    "            if 'eval_loss' in log and final_eval_loss is None:\n",
    "                final_eval_loss = log['eval_loss']\n",
    "            if final_train_loss is not None and final_eval_loss is not None:\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n📊 FINAL METRICS:\")\n",
    "        if final_train_loss:\n",
    "            print(f\"   Final training loss: {final_train_loss:.4f}\")\n",
    "        if final_eval_loss:\n",
    "            print(f\"   Final validation loss: {final_eval_loss:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    print(f\"\\n💾 Saving fine-tuned model...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"✅ Model saved to: {output_dir}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n⏹️  Training interrupted by user\")\n",
    "    print(f\"   Partial model may be saved in {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training failed: {e}\")\n",
    "    print(f\"💡 Common issues:\")\n",
    "    print(f\"   • Out of memory: Reduce batch size or use gradient checkpointing\")\n",
    "    print(f\"   • Model too large: Use smaller model or more aggressive LoRA settings\")\n",
    "    print(f\"   • Device issues: Check CUDA/MPS availability\")\n",
    "    \n",
    "    # Try to save partial progress\n",
    "    try:\n",
    "        trainer.save_model(output_dir + \"_partial\")\n",
    "        print(f\"📁 Partial model saved to: {output_dir}_partial\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\n🏁 Training session complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test the Fine-Tuned Model\n",
    "\n",
    "Let's test our fine-tuned model to see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTING FINE-TUNED MODEL\n",
      "==================================================\n",
      "✅ Using model from training session\n",
      "\n",
      "🔍 TESTING WITH SAMPLE PROMPTS:\n",
      "========================================\n",
      "\n",
      "📝 TEST 1:\n",
      "Prompt: Explain what deep learning is in simple terms.\n",
      "------------------------------\n",
      "Response: Deep learning is like teaching a computer to learn from data, without explicitly telling it *how* to do things. Instead of giving the computer step-by-step instructions, you show it lots of examples and let it figure out the patterns itself.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **Data:** You feed the computer tons of information – images, text, audio, etc. This is your training data.\n",
      "*   **Neural Networks:**  Deep learning uses something called neural networks - these are complex systems inspired by how the human brain works. They consist of layers of interconnected \"neurons.\"\n",
      "*   **Learning:** The computer adjusts the connections between neurons based on its mistakes. If it gets an incorrect answer, it tweaks those\n",
      "\n",
      "📝 TEST 2:\n",
      "Prompt: What are the benefits of eating vegetables?\n",
      "------------------------------\n",
      "Response: Vegetables offer a plethora of benefits for your health. Here's a breakdown of some key advantages:\n",
      "\n",
      "*   **Rich in Vitamins and Minerals:** They're packed with essential vitamins like Vitamin C, A, K, and folate, as well as minerals like potassium, magnesium, and iron. These nutrients play vital roles in various bodily functions.\n",
      "\n",
      "*   **High in Fiber:** Vegetables are excellent sources of dietary fiber, which aids digestion, promotes gut health, and can help regulate blood sugar levels.\n",
      "\n",
      "*   **Antioxidant Powerhouse:** Many vegetables contain powerful antioxidants that protect cells from damage caused by free radicals. This helps reduce the risk of chronic diseases.\n",
      "\n",
      "*   **Low in Calories:** Compared to many other\n",
      "\n",
      "📝 TEST 3:\n",
      "Prompt: How do you learn a new programming language?\n",
      "------------------------------\n",
      "Response: >\n",
      "\n",
      "Learning a new programming language is a rewarding but challenging process. Here’s a breakdown of how to approach it, broken down into manageable steps:\n",
      "\n",
      "**1. Start with the Basics (Fundamentals):**\n",
      "\n",
      "*   **Understand Data Types:**  Learn about integers, floats, strings, booleans, and lists/arrays. Know their characteristics and when to use them.\n",
      "*   **Variables & Operators:** Master how to declare variables, assign values to them, and understand basic operators like arithmetic (`+`, `-`, `*`, `/`), comparison (`==`, `!=`, `>`, `<`, `>=`, `<=`), and logical (`and`, `or`, `not`).\n",
      "*   **Control Flow:**  Crucially, grasp the concepts of\n",
      "\n",
      "📊 EVALUATION NOTES:\n",
      "• Compare responses to the original Gemma 3 1B Instruct model behavior\n",
      "• Look for improved instruction following\n",
      "• Check if responses are more relevant to your specific domain\n",
      "• With more training data and epochs, quality should improve\n",
      "\n",
      "✅ Model testing complete!\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model\n",
    "print(\"🧪 TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the fine-tuned model if needed\n",
    "if 'trainer' in locals() and trainer.model is not None:\n",
    "    fine_tuned_model = trainer.model\n",
    "    print(\"✅ Using model from training session\")\n",
    "else:\n",
    "    # Load from saved checkpoint\n",
    "    print(\"📥 Loading fine-tuned model from disk...\")\n",
    "    try:\n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load LoRA weights\n",
    "        fine_tuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "        fine_tuned_model = fine_tuned_model.to(device)\n",
    "        print(\"✅ Fine-tuned model loaded successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading fine-tuned model: {e}\")\n",
    "        print(\"💡 Using original model for comparison\")\n",
    "        fine_tuned_model = model\n",
    "\n",
    "# Set model to evaluation mode\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"### Instruction:\\nExplain what deep learning is in simple terms.\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nWhat are the benefits of eating vegetables?\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nHow do you learn a new programming language?\\n\\n### Response:\\n\"\n",
    "]\n",
    "\n",
    "print(\"\\n🔍 TESTING WITH SAMPLE PROMPTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n📝 TEST {i}:\")\n",
    "    print(f\"Prompt: {prompt.split('### Response:')[0].split('### Instruction:')[1].strip()}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = fine_tuned_model.generate(\n",
    "                inputs,\n",
    "                max_length=inputs.shape[1] + 150,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_only = full_response[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"Response: {response_only}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating response: {e}\")\n",
    "\n",
    "print(\"\\n📊 EVALUATION NOTES:\")\n",
    "print(\"• Compare responses to the original Gemma 3 1B Instruct model behavior\")\n",
    "print(\"• Look for improved instruction following\")\n",
    "print(\"• Check if responses are more relevant to your specific domain\")\n",
    "print(\"• With more training data and epochs, quality should improve\")\n",
    "\n",
    "print(\"\\n✅ Model testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save and Share Your Model\n",
    "\n",
    "Let's prepare the model for sharing and future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SAVING AND PREPARING MODEL\n",
      "==================================================\n",
      "📝 Model card created\n",
      "⚙️ Training configuration saved\n",
      "\n",
      "📁 SAVED FILES IN ./gemma-3-1b-it-finetuned:\n",
      "   README.md (0.0 MB)\n",
      "   training_config.json (0.0 MB)\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "1. Test the model thoroughly with your use case\n",
      "2. If performance is good, consider training with more data\n",
      "3. You can push to HuggingFace Hub for sharing:\n",
      "   model.push_to_hub('your-username/gemma-3-1b-it-finetuned')\n",
      "4. Or share the './gemma-3-1b-it-finetuned' folder directly\n",
      "\n",
      "✅ Model preparation complete!\n"
     ]
    }
   ],
   "source": [
    "# Save and prepare model for sharing\n",
    "print(\"💾 SAVING AND PREPARING MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create model card with training information\n",
    "model_card_content = f\"\"\"\n",
    "# Gemma 3 1B Instruct Fine-tuned Model\n",
    "\n",
    "## Model Description\n",
    "This is a fine-tuned version of Google's Gemma 3 1B Instruct model, adapted for custom instruction-following tasks.\n",
    "\n",
    "## Training Details\n",
    "- **Base model**: {MODEL_NAME}\n",
    "- **Fine-tuning method**: LoRA (Low-Rank Adaptation)\n",
    "- **Training device**: {device}\n",
    "- **LoRA rank**: {LORA_R}\n",
    "- **LoRA alpha**: {LORA_ALPHA}\n",
    "- **Training epochs**: {training_args.num_train_epochs}\n",
    "- **Learning rate**: {training_args.learning_rate}\n",
    "- **Batch size**: {effective_batch_size} (effective)\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/this/model\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"### Instruction:\\\\nYour question here\\\\n\\\\n### Response:\\\\n\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=200, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "## Training Data\n",
    "The model was fine-tuned on a custom instruction-following dataset.\n",
    "\n",
    "## Limitations\n",
    "- This is a demonstration model with limited training data\n",
    "- May not generalize well to all tasks\n",
    "- Requires the same format for optimal performance\n",
    "\n",
    "## License\n",
    "This model inherits the license from the base Gemma 3 model.\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(f\"{output_dir}/README.md\", \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "\n",
    "print(\"📝 Model card created\")\n",
    "\n",
    "# Save training configuration\n",
    "training_config = {\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"device\": device,\n",
    "    \"lora_config\": {\n",
    "        \"r\": LORA_R,\n",
    "        \"alpha\": LORA_ALPHA,\n",
    "        \"dropout\": LORA_DROPOUT,\n",
    "        \"target_modules\": target_modules\n",
    "    },\n",
    "    \"training_args\": training_args.to_dict() if hasattr(training_args, 'to_dict') else str(training_args),\n",
    "    \"dataset_size\": len(extended_data),\n",
    "    \"max_length\": MAX_LENGTH\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2, default=str)\n",
    "\n",
    "print(\"⚙️ Training configuration saved\")\n",
    "\n",
    "# List all saved files\n",
    "print(f\"\\n📁 SAVED FILES IN {output_dir}:\")\n",
    "try:\n",
    "    for file in os.listdir(output_dir):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"   {file} ({size_mb:.1f} MB)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error listing files: {e}\")\n",
    "\n",
    "# Instructions for using the model\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(f\"1. Test the model thoroughly with your use case\")\n",
    "print(f\"2. If performance is good, consider training with more data\")\n",
    "print(f\"3. You can push to HuggingFace Hub for sharing:\")\n",
    "print(f\"   model.push_to_hub('your-username/gemma-3-1b-it-finetuned')\")\n",
    "print(f\"4. Or share the '{output_dir}' folder directly\")\n",
    "\n",
    "print(f\"\\n✅ Model preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully completed the fine-tuning process for Gemma 3 1B Instruct. Here's what you've accomplished:\n",
    "\n",
    "### ✅ What you achieved:\n",
    "1. **Environment Setup**: Configured the system for different devices (CPU, CUDA, Apple Silicon)\n",
    "2. **Model Loading**: Successfully loaded and prepared Gemma 3 1B Instruct for fine-tuning\n",
    "3. **Dataset Preparation**: Created and formatted training data for instruction-following\n",
    "4. **LoRA Implementation**: Applied efficient fine-tuning with Low-Rank Adaptation\n",
    "5. **Training Execution**: Ran the complete fine-tuning process\n",
    "6. **Model Evaluation**: Tested the fine-tuned model's performance\n",
    "7. **Model Deployment**: Saved and prepared the model for sharing\n",
    "\n",
    "### 🔑 Key concepts learned:\n",
    "- **Parameter-Efficient Fine-Tuning**: Using LoRA to reduce computational requirements\n",
    "- **Device Optimization**: Configuring training for different hardware\n",
    "- **Dataset Formatting**: Preparing instruction-following datasets\n",
    "- **Training Monitoring**: Understanding metrics and performance\n",
    "- **Model Evaluation**: Testing and validating fine-tuned models\n",
    "\n",
    "### 🚀 Improvement strategies:\n",
    "\n",
    "#### For better results:\n",
    "1. **More Training Data**: Use 1000+ high-quality examples\n",
    "2. **Longer Training**: Increase epochs and fine-tune learning rate\n",
    "3. **Better Data Quality**: Clean, diverse, and relevant examples\n",
    "4. **Hyperparameter Tuning**: Experiment with LoRA rank, learning rate, batch size\n",
    "5. **Evaluation Metrics**: Implement proper evaluation beyond loss\n",
    "\n",
    "#### Advanced techniques:\n",
    "1. **QLoRA**: Quantized LoRA for even more efficiency\n",
    "2. **Multi-task Training**: Train on multiple tasks simultaneously\n",
    "3. **Reinforcement Learning from Human Feedback (RLHF)**: Align with human preferences\n",
    "4. **Curriculum Learning**: Progressive training difficulty\n",
    "5. **Model Merging**: Combine multiple fine-tuned adapters\n",
    "\n",
    "### 💡 Production considerations:\n",
    "- **Quantization**: Use 8-bit or 4-bit quantization for deployment\n",
    "- **Optimization**: ONNX conversion or TensorRT for inference speed\n",
    "- **Monitoring**: Track model performance in production\n",
    "- **Safety**: Implement content filtering and bias detection\n",
    "- **Versioning**: Keep track of model versions and training data\n",
    "\n",
    "### 🛠️ Troubleshooting tips:\n",
    "- **Memory Issues**: Reduce batch size, use gradient checkpointing, or try CPU training\n",
    "- **Slow Training**: Check device utilization, use mixed precision, optimize data loading\n",
    "- **Poor Performance**: Increase training data, adjust learning rate, check data quality\n",
    "- **Overfitting**: Use validation split, early stopping, or regularization\n",
    "\n",
    "### 📚 Further learning:\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [Gemma Model Documentation](https://huggingface.co/docs/transformers/model_doc/gemma)\n",
    "- [Fine-tuning Best Practices](https://huggingface.co/blog/rlhf)\n",
    "\n",
    "### 🎉 Congratulations!\n",
    "You now have a working fine-tuned Gemma 3 1B Instruct model and the knowledge to improve it further. The techniques you've learned can be applied to other models and tasks. Happy fine-tuning! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
