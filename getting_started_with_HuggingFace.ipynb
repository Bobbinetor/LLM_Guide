{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Local LLMs: Your First Steps 🚀\n",
    "\n",
    "Welcome to the world of local Large Language Models (LLMs)! This notebook will guide you through running your first LLM locally using HuggingFace Transformers, helping you understand each component and concept.\n",
    "\n",
    "## What you'll learn:\n",
    "- What are the core components of a local LLM setup\n",
    "- How to install and configure the necessary libraries\n",
    "- Load your first model from HuggingFace\n",
    "- Generate text and understand the process\n",
    "- Compare different models and configurations\n",
    "- Understand tokenization and model parameters\n",
    "\n",
    "## Prerequisites:\n",
    "- Python 3.8 or higher\n",
    "- At least 8GB of RAM (16GB+ recommended)\n",
    "- Basic understanding of Python\n",
    "- Stable internet connection for initial model download\n",
    "- A CUDA-based GPU is strongly recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding the Components\n",
    "\n",
    "Before we start coding, let's understand the key components involved in running LLMs locally:\n",
    "\n",
    "### 🧠 **Large Language Model (LLM)**\n",
    "A neural network trained on vast amounts of text to understand and generate human-like text.\n",
    "\n",
    "### 🔤 **Tokenizer**\n",
    "Converts text into numbers (tokens) that the model can understand, and vice versa.\n",
    "\n",
    "### 🏗️ **Model Architecture**\n",
    "The structure of the neural network (e.g., GPT, BERT, T5).\n",
    "\n",
    "### ⚙️ **Generation Parameters**\n",
    "Settings that control how text is generated (temperature, top-k, max length, etc.).\n",
    "\n",
    "### 🤗 **HuggingFace Hub**\n",
    "A platform hosting thousands of pre-trained models you can download and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Required Libraries\n",
    "\n",
    "First, let's install the necessary libraries. Run this cell to install everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing required packages...\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.12/site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.18-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2025.9.18-cp312-cp312-macosx_11_0_arm64.whl (287 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: typing-extensions, tqdm, safetensors, regex, pyyaml, hf-xet, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.0 pyyaml-6.0.2 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.56.1 typing-extensions-4.15.0\n",
      "✅ transformers installed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./myenv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.8.0-cp312-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, setuptools, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 setuptools-80.9.0 sympy-1.14.0 torch-2.8.0\n",
      "✅ torch installed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./myenv/lib/python3.12/site-packages (from accelerate) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./myenv/lib/python3.12/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in ./myenv/lib/python3.12/site-packages (from accelerate) (0.35.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./myenv/lib/python3.12/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.1\n",
      "✅ accelerate installed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "✅ sentencepiece installed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (7.1.0)\n",
      "✅ psutil installed successfully\n",
      "\n",
      "🎉 Installation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "# Run this cell if you haven't installed these packages yet\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages\n",
    "packages = [\n",
    "    \"transformers\",      # HuggingFace transformers library\n",
    "    \"torch\",            # PyTorch for model computations\n",
    "    \"accelerate\",       # For optimized model loading\n",
    "    \"sentencepiece\",    # For certain tokenizers\n",
    "    \"psutil\",           # For system monitoring\n",
    "]\n",
    "\n",
    "print(\"📦 Installing required packages...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        install_package(package)\n",
    "        print(f\"✅ {package} installed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to install {package}: {e}\")\n",
    "\n",
    "print(\"\\n🎉 Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries and Check Environment\n",
    "\n",
    "Let's import the necessary libraries and check our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alfredopetruolo/Desktop/Code/ollama_guide/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️  SYSTEM INFORMATION\n",
      "==================================================\n",
      "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n",
      "PyTorch version: 2.8.0\n",
      "Available RAM: 24.0 GB\n",
      "Available disk space: 472.9 GB\n",
      "🚀 Apple Silicon GPU (MPS) available\n",
      "\n",
      "🎯 Selected device: mps\n",
      "\n",
      "✅ Environment check complete!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import time\n",
    "import psutil\n",
    "import warnings\n",
    "\n",
    "# Suppress some warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Check system information\n",
    "print(\"🖥️  SYSTEM INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"Available disk space: {psutil.disk_usage('/').free / (1024**3):.1f} GB\")\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🚀 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"🚀 Apple Silicon GPU (MPS) available\")\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    print(\"💻 Using CPU (GPU not available)\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\n🎯 Selected device: {device}\")\n",
    "print(\"\\n✅ Environment check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Choose and Load Your First Model\n",
    "\n",
    "For your first experience, we'll use a small, efficient model. We'll start with **distilgpt2** - a smaller, faster version of GPT-2 that's perfect for beginners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model: distilgpt2\n",
      "⏳ This may take a few minutes on first run (downloading model)...\n",
      "\n",
      "🔤 Loading tokenizer...\n",
      "✅ Tokenizer loaded successfully\n",
      "🧠 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully\n",
      "\n",
      "⏱️  Total loading time: 185.65 seconds\n",
      "📊 Model parameters: 81,912,576\n",
      "💾 Model size: ~0.2 GB\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"distilgpt2\"  # Small, fast model for beginners\n",
    "# Alternative models you can try:\n",
    "# MODEL_NAME = \"gpt2\"                    # Classic GPT-2 base model\n",
    "# MODEL_NAME = \"microsoft/DialoGPT-small\" # Conversational model\n",
    "# MODEL_NAME = \"facebook/opt-125m\"        # Meta's OPT model (125M parameters)\n",
    "\n",
    "print(f\"📥 Loading model: {MODEL_NAME}\")\n",
    "print(\"⏳ This may take a few minutes on first run (downloading model)...\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    print(\"🔤 Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Add padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"✅ Tokenizer loaded successfully\")\n",
    "    \n",
    "    # Load model\n",
    "    print(\"🧠 Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "        device_map=\"auto\" if device != \"cpu\" else None,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    if device == \"cpu\":\n",
    "        model = model.to(device)\n",
    "    \n",
    "    print(\"✅ Model loaded successfully\")\n",
    "    \n",
    "    # Calculate loading time\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n⏱️  Total loading time: {load_time:.2f} seconds\")\n",
    "    print(f\"📊 Model parameters: {model.num_parameters():,}\")\n",
    "    print(f\"💾 Model size: ~{model.num_parameters() * 2 / 1e9:.1f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"💡 Try using a different model or check your internet connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Understanding Tokenization\n",
    "\n",
    "Before generating text, let's understand how tokenization works - the process of converting text to numbers that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 TOKENIZATION DEMONSTRATION\n",
      "==================================================\n",
      "Original text: 'Hello! How are you doing today?'\n",
      "\n",
      "Token IDs: [15496, 0, 1374, 389, 345, 1804, 1909, 30]\n",
      "Number of tokens: 8\n",
      "\n",
      "Individual tokens:\n",
      "  0: 15496 → 'Hello'\n",
      "  1: 0 → '!'\n",
      "  2: 1374 → ' How'\n",
      "  3: 389 → ' are'\n",
      "  4: 345 → ' you'\n",
      "  5: 1804 → ' doing'\n",
      "  6: 1909 → ' today'\n",
      "  7: 30 → '?'\n",
      "\n",
      "Decoded back to text: 'Hello! How are you doing today?'\n",
      "\n",
      "📚 Tokenizer vocabulary size: 50,257 tokens\n",
      "\n",
      "💡 Key takeaways:\n",
      "   - Text is split into subword tokens\n",
      "   - Each token has a unique numerical ID\n",
      "   - The model works with these IDs, not raw text\n",
      "   - Common words are usually single tokens\n",
      "   - Rare words might be split into multiple tokens\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of tokenization\n",
    "sample_text = \"Hello! How are you doing today?\"\n",
    "\n",
    "print(\"🔤 TOKENIZATION DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original text: '{sample_text}'\")\n",
    "print()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print()\n",
    "\n",
    "# Show individual tokens\n",
    "print(\"Individual tokens:\")\n",
    "for i, token_id in enumerate(tokens):\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"  {i}: {token_id} → '{token_text}'\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Demonstrate decoding\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "print(f\"Decoded back to text: '{decoded_text}'\")\n",
    "\n",
    "# Show vocabulary size\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"\\n📚 Tokenizer vocabulary size: {vocab_size:,} tokens\")\n",
    "\n",
    "print(\"\\n💡 Key takeaways:\")\n",
    "print(\"   - Text is split into subword tokens\")\n",
    "print(\"   - Each token has a unique numerical ID\")\n",
    "print(\"   - The model works with these IDs, not raw text\")\n",
    "print(\"   - Common words are usually single tokens\")\n",
    "print(\"   - Rare words might be split into multiple tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Your First Text Generation\n",
    "\n",
    "Now let's generate our first text! We'll start with a simple prompt and understand each step of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Generating text from prompt: 'The future of artificial intelligence is'\n",
      "⚙️  Parameters: max_length=80, temperature=0.7, do_sample=True\n",
      "--------------------------------------------------\n",
      "📝 Prompt tokens: 6\n",
      "✅ Generation complete!\n",
      "⏱️  Time taken: 6.07 seconds\n",
      "📊 New tokens generated: 74\n",
      "🚀 Generation speed: 12.2 tokens/second\n",
      "\n",
      "📝 COMPLETE OUTPUT:\n",
      "==============================\n",
      "Prompt: The future of artificial intelligence is\n",
      "Generated: not yet clear. However, in an interview with the New York Times, David O'Keefe, director of the National Security Agency, said, \"You can't predict if and how this could change.\"\n",
      "==============================\n",
      "Full text: The future of artificial intelligence is not yet clear. However, in an interview with the New York Times, David O'Keefe, director of the National Security Agency, said, \"You can't predict if and how this could change.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=50, temperature=0.7, do_sample=True):\n",
    "    \"\"\"\n",
    "    Generate text using our loaded model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input text to continue\n",
    "        max_length (int): Maximum total length (prompt + generated)\n",
    "        temperature (float): Controls randomness (0.1 = conservative, 1.0 = creative)\n",
    "        do_sample (bool): Whether to use sampling or greedy decoding\n",
    "    \"\"\"\n",
    "    print(f\"🤖 Generating text from prompt: '{prompt}'\")\n",
    "    print(f\"⚙️  Parameters: max_length={max_length}, temperature={temperature}, do_sample={do_sample}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Encode the prompt\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        print(f\"📝 Prompt tokens: {len(input_ids[0])}\")\n",
    "        \n",
    "        # Generate text\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                attention_mask=input_ids.ne(tokenizer.pad_token_id)\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the newly generated part\n",
    "        new_text = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        total_tokens = len(outputs[0])\n",
    "        new_tokens = total_tokens - len(input_ids[0])\n",
    "        \n",
    "        print(f\"✅ Generation complete!\")\n",
    "        print(f\"⏱️  Time taken: {generation_time:.2f} seconds\")\n",
    "        print(f\"📊 New tokens generated: {new_tokens}\")\n",
    "        print(f\"🚀 Generation speed: {new_tokens/generation_time:.1f} tokens/second\")\n",
    "        print()\n",
    "        print(\"📝 COMPLETE OUTPUT:\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated: {new_text}\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Full text: {generated_text}\")\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during generation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with a simple prompt\n",
    "first_prompt = \"The future of artificial intelligence is\"\n",
    "result = generate_text(first_prompt, max_length=80, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Understanding Generation Parameters\n",
    "\n",
    "Let's experiment with different parameters to see how they affect the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 PARAMETER EXPERIMENTS\n",
      "============================================================\n",
      "Testing prompt: 'Once upon a time in a magical forest'\n",
      "\n",
      "🥶 EXPERIMENT 1: Low Temperature (Conservative)\n",
      "🤖 Generating text from prompt: 'Once upon a time in a magical forest'\n",
      "⚙️  Parameters: max_length=70, temperature=0.1, do_sample=True\n",
      "--------------------------------------------------\n",
      "📝 Prompt tokens: 8\n",
      "✅ Generation complete!\n",
      "⏱️  Time taken: 1.25 seconds\n",
      "📊 New tokens generated: 62\n",
      "🚀 Generation speed: 49.5 tokens/second\n",
      "\n",
      "📝 COMPLETE OUTPUT:\n",
      "==============================\n",
      "Prompt: Once upon a time in a magical forest\n",
      "Generated: , a man is born.\n",
      "==============================\n",
      "Full text: Once upon a time in a magical forest, a man is born.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔥 EXPERIMENT 2: High Temperature (Creative)\n",
      "🤖 Generating text from prompt: 'Once upon a time in a magical forest'\n",
      "⚙️  Parameters: max_length=70, temperature=1.2, do_sample=True\n",
      "--------------------------------------------------\n",
      "📝 Prompt tokens: 8\n",
      "✅ Generation complete!\n",
      "⏱️  Time taken: 0.78 seconds\n",
      "📊 New tokens generated: 62\n",
      "🚀 Generation speed: 79.8 tokens/second\n",
      "\n",
      "📝 COMPLETE OUTPUT:\n",
      "==============================\n",
      "Prompt: Once upon a time in a magical forest\n",
      "Generated: , there was only a single piece of stone and, of course, was silver. But… there was no silver. No silver. No golden. And instead of it, she could only look around in the sky for silver, with no silver that had remained. She knew full well that she would lose her\n",
      "==============================\n",
      "Full text: Once upon a time in a magical forest, there was only a single piece of stone and, of course, was silver. But… there was no silver. No silver. No golden. And instead of it, she could only look around in the sky for silver, with no silver that had remained. She knew full well that she would lose her\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎯 EXPERIMENT 3: Greedy Decoding (Deterministic)\n",
      "🤖 Generating text from prompt: 'Once upon a time in a magical forest'\n",
      "⚙️  Parameters: max_length=70, temperature=1.0, do_sample=False\n",
      "--------------------------------------------------\n",
      "📝 Prompt tokens: 8\n",
      "✅ Generation complete!\n",
      "⏱️  Time taken: 0.56 seconds\n",
      "📊 New tokens generated: 62\n",
      "🚀 Generation speed: 111.7 tokens/second\n",
      "\n",
      "📝 COMPLETE OUTPUT:\n",
      "==============================\n",
      "Prompt: Once upon a time in a magical forest\n",
      "Generated: , the world is filled with monsters.\n",
      "==============================\n",
      "Full text: Once upon a time in a magical forest, the world is filled with monsters.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "📚 PARAMETER EXPLANATION:\n",
      "----------------------------------------\n",
      "🌡️  Temperature:\n",
      "   • Low (0.1-0.5): More predictable, focused outputs\n",
      "   • Medium (0.6-0.9): Balanced creativity and coherence\n",
      "   • High (1.0+): More creative but potentially less coherent\n",
      "\n",
      "🎲 Sampling:\n",
      "   • do_sample=True: Uses temperature and randomness\n",
      "   • do_sample=False: Greedy decoding (always picks most likely token)\n",
      "\n",
      "📏 Max Length:\n",
      "   • Controls total output length (prompt + generated text)\n",
      "   • Longer = more content but slower generation\n"
     ]
    }
   ],
   "source": [
    "# Test different parameters with the same prompt\n",
    "test_prompt = \"Once upon a time in a magical forest\"\n",
    "\n",
    "print(\"🧪 PARAMETER EXPERIMENTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Testing prompt: '{test_prompt}'\")\n",
    "print()\n",
    "\n",
    "# Experiment 1: Low temperature (conservative)\n",
    "print(\"🥶 EXPERIMENT 1: Low Temperature (Conservative)\")\n",
    "generate_text(test_prompt, max_length=70, temperature=0.1)\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Experiment 2: High temperature (creative)\n",
    "print(\"🔥 EXPERIMENT 2: High Temperature (Creative)\")\n",
    "generate_text(test_prompt, max_length=70, temperature=1.2)\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Experiment 3: Greedy decoding (deterministic)\n",
    "print(\"🎯 EXPERIMENT 3: Greedy Decoding (Deterministic)\")\n",
    "generate_text(test_prompt, max_length=70, temperature=1.0, do_sample=False)\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "print(\"📚 PARAMETER EXPLANATION:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"🌡️  Temperature:\")\n",
    "print(\"   • Low (0.1-0.5): More predictable, focused outputs\")\n",
    "print(\"   • Medium (0.6-0.9): Balanced creativity and coherence\")\n",
    "print(\"   • High (1.0+): More creative but potentially less coherent\")\n",
    "print()\n",
    "print(\"🎲 Sampling:\")\n",
    "print(\"   • do_sample=True: Uses temperature and randomness\")\n",
    "print(\"   • do_sample=False: Greedy decoding (always picks most likely token)\")\n",
    "print()\n",
    "print(\"📏 Max Length:\")\n",
    "print(\"   • Controls total output length (prompt + generated text)\")\n",
    "print(\"   • Longer = more content but slower generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Using HuggingFace Pipelines (Simplified Approach)\n",
    "\n",
    "HuggingFace also provides a simpler way to use models through pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚰 Creating HuggingFace Pipeline...\n",
      "✅ Pipeline created successfully!\n",
      "🎯 Pipeline device: mps\n",
      "\n",
      "🤖 Pipeline generation with prompt: 'The benefits of running AI models locally include'\n",
      "--------------------------------------------------\n",
      "⏱️  Generation time: 9.22 seconds\n",
      "\n",
      "📝 OUTPUT 1:\n",
      "--------------------\n",
      "The benefits of running AI models locally include:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "📝 OUTPUT 2:\n",
      "--------------------\n",
      "The benefits of running AI models locally include a more sophisticated and better understanding of the effects of network connectivity and information exchange, as well as the benefits of a networked network with a larger audience.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "💡 Pipeline advantages:\n",
      "   • Simpler API\n",
      "   • Automatic handling of tokenization\n",
      "   • Built-in optimizations\n",
      "   • Easy to generate multiple outputs\n",
      "   • Works seamlessly across different devices (CPU, CUDA, MPS)\n"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline\n",
    "print(\"🚰 Creating HuggingFace Pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Create pipeline with proper device handling\n",
    "    if device == \"cuda\":\n",
    "        # For CUDA, specify GPU device\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=0,  # GPU device\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        # For Apple Silicon, don't specify device as model is already on MPS\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        # For CPU\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=-1,  # CPU\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "    \n",
    "    print(\"✅ Pipeline created successfully!\")\n",
    "    print(f\"🎯 Pipeline device: {device}\")\n",
    "    print()\n",
    "    \n",
    "    # Generate text using the pipeline\n",
    "    pipeline_prompt = \"The benefits of running AI models locally include\"\n",
    "    \n",
    "    print(f\"🤖 Pipeline generation with prompt: '{pipeline_prompt}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate multiple outputs\n",
    "    outputs = generator(\n",
    "        pipeline_prompt,\n",
    "        max_length=100,\n",
    "        num_return_sequences=2,  # Generate 2 different outputs\n",
    "        temperature=0.8,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"⏱️  Generation time: {generation_time:.2f} seconds\")\n",
    "    print()\n",
    "    \n",
    "    for i, output in enumerate(outputs, 1):\n",
    "        print(f\"📝 OUTPUT {i}:\")\n",
    "        print(\"-\" * 20)\n",
    "        print(output['generated_text'])\n",
    "        print()\n",
    "    \n",
    "    print(\"💡 Pipeline advantages:\")\n",
    "    print(\"   • Simpler API\")\n",
    "    print(\"   • Automatic handling of tokenization\")\n",
    "    print(\"   • Built-in optimizations\")\n",
    "    print(\"   • Easy to generate multiple outputs\")\n",
    "    print(\"   • Works seamlessly across different devices (CPU, CUDA, MPS)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating pipeline: {e}\")\n",
    "    print(\"💡 Troubleshooting tips:\")\n",
    "    print(\"   • Model might be loaded with accelerate\")\n",
    "    print(\"   • Try reloading the model without device_map='auto'\")\n",
    "    print(\"   • For Apple Silicon, ensure you have the latest PyTorch with MPS support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Performance Measurement\n",
    "\n",
    "Let's measure the performance of our model and understand resource usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 PERFORMANCE MEASUREMENT\n",
      "==================================================\n",
      "Model: distilgpt2\n",
      "Device: mps\n",
      "Test prompt: 'The key to successful machine learning is'\n",
      "Number of runs: 3\n",
      "\n",
      "🏃 Run 1/3...\n",
      "   Time: 1.05s, Tokens: 30\n",
      "🏃 Run 2/3...\n",
      "   Time: 0.34s, Tokens: 30\n",
      "🏃 Run 3/3...\n",
      "   Time: 0.34s, Tokens: 30\n",
      "\n",
      "📈 PERFORMANCE RESULTS:\n",
      "   Average time: 0.57 seconds\n",
      "   Average tokens generated: 30.0\n",
      "   Average speed: 52.3 tokens/second\n",
      "   Fastest run: 0.34 seconds\n",
      "   Slowest run: 1.05 seconds\n",
      "\n",
      "📋 MODEL INFORMATION:\n",
      "   Name: distilgpt2\n",
      "   Parameters: 81,912,576\n",
      "   Running on: mps\n",
      "   Tokenizer vocab size: 50,257\n"
     ]
    }
   ],
   "source": [
    "def measure_model_performance(prompt, runs=3):\n",
    "    \"\"\"\n",
    "    Measure the performance of our current model.\n",
    "    \"\"\"\n",
    "    print(f\"📊 PERFORMANCE MEASUREMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Test prompt: '{prompt}'\")\n",
    "    print(f\"Number of runs: {runs}\")\n",
    "    print()\n",
    "    \n",
    "    times = []\n",
    "    token_counts = []\n",
    "    \n",
    "    for run in range(runs):\n",
    "        print(f\"🏃 Run {run + 1}/{runs}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=len(input_ids[0]) + 30,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            generation_time = end_time - start_time\n",
    "            \n",
    "            new_tokens = len(outputs[0]) - len(input_ids[0])\n",
    "            \n",
    "            times.append(generation_time)\n",
    "            token_counts.append(new_tokens)\n",
    "            \n",
    "            print(f\"   Time: {generation_time:.2f}s, Tokens: {new_tokens}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error in run {run + 1}: {e}\")\n",
    "    \n",
    "    if times:\n",
    "        avg_time = sum(times) / len(times)\n",
    "        avg_tokens = sum(token_counts) / len(token_counts)\n",
    "        avg_speed = avg_tokens / avg_time\n",
    "        \n",
    "        print(f\"\\n📈 PERFORMANCE RESULTS:\")\n",
    "        print(f\"   Average time: {avg_time:.2f} seconds\")\n",
    "        print(f\"   Average tokens generated: {avg_tokens:.1f}\")\n",
    "        print(f\"   Average speed: {avg_speed:.1f} tokens/second\")\n",
    "        print(f\"   Fastest run: {min(times):.2f} seconds\")\n",
    "        print(f\"   Slowest run: {max(times):.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            \"avg_time\": avg_time,\n",
    "            \"avg_tokens\": avg_tokens,\n",
    "            \"avg_speed\": avg_speed\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Measure performance\n",
    "perf_prompt = \"The key to successful machine learning is\"\n",
    "performance_results = measure_model_performance(perf_prompt, runs=3)\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\n📋 MODEL INFORMATION:\")\n",
    "print(f\"   Name: {MODEL_NAME}\")\n",
    "print(f\"   Parameters: {model.num_parameters():,}\")\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    print(f\"   GPU memory usage: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(f\"   Running on: {device}\")\n",
    "print(f\"   Tokenizer vocab size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Understanding Different Model Types\n",
    "\n",
    "Let's explore different types of models you can use locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 UNDERSTANDING DIFFERENT MODEL TYPES\n",
      "============================================================\n",
      "\n",
      "🏷️  Causal Language Models (GPT-style)\n",
      "   📝 Description: Generate text by predicting the next token\n",
      "   🔧 Examples: gpt2, distilgpt2, microsoft/DialoGPT-small\n",
      "   💼 Use cases: Text generation, Chatbots, Creative writing\n",
      "   ✅ Pros: Great for open-ended generation, Conversational\n",
      "   ⚠️  Cons: Can be repetitive, May generate false information\n",
      "\n",
      "🏷️  Instruction-Tuned Models\n",
      "   📝 Description: Trained to follow instructions and be helpful\n",
      "   🔧 Examples: facebook/opt-iml-1.3b, EleutherAI/gpt-j-6B\n",
      "   💼 Use cases: Question answering, Task assistance, Chatbots\n",
      "   ✅ Pros: Better at following instructions, More helpful responses\n",
      "   ⚠️  Cons: May be less creative, Can be overly cautious\n",
      "\n",
      "🏷️  Specialized Models\n",
      "   📝 Description: Trained for specific tasks or domains\n",
      "   🔧 Examples: bert-base-uncased, facebook/bart-large-cnn\n",
      "   💼 Use cases: Text classification, Summarization, Translation\n",
      "   ✅ Pros: Excellent at specific tasks, Often smaller and faster\n",
      "   ⚠️  Cons: Limited to specific use cases, Not for general chat\n",
      "\n",
      "🎯 CHOOSING THE RIGHT MODEL:\n",
      "----------------------------------------\n",
      "For beginners, start with:\n",
      "• distilgpt2 - Fastest, smallest GPT-2 variant\n",
      "• gpt2 - Classic text generation model\n",
      "• microsoft/DialoGPT-small - Conversational AI\n",
      "\n",
      "Consider these factors:\n",
      "• Model size (larger = better quality, slower speed)\n",
      "• Your hardware (RAM, GPU availability)\n",
      "• Intended use case (chat, generation, specific tasks)\n",
      "• Download time and storage space\n"
     ]
    }
   ],
   "source": [
    "# Information about different model types\n",
    "print(\"🧠 UNDERSTANDING DIFFERENT MODEL TYPES\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "model_info = {\n",
    "    \"Causal Language Models (GPT-style)\": {\n",
    "        \"description\": \"Generate text by predicting the next token\",\n",
    "        \"examples\": [\"gpt2\", \"distilgpt2\", \"microsoft/DialoGPT-small\"],\n",
    "        \"use_cases\": [\"Text generation\", \"Chatbots\", \"Creative writing\"],\n",
    "        \"pros\": [\"Great for open-ended generation\", \"Conversational\"],\n",
    "        \"cons\": [\"Can be repetitive\", \"May generate false information\"]\n",
    "    },\n",
    "    \"Instruction-Tuned Models\": {\n",
    "        \"description\": \"Trained to follow instructions and be helpful\",\n",
    "        \"examples\": [\"facebook/opt-iml-1.3b\", \"EleutherAI/gpt-j-6B\"],\n",
    "        \"use_cases\": [\"Question answering\", \"Task assistance\", \"Chatbots\"],\n",
    "        \"pros\": [\"Better at following instructions\", \"More helpful responses\"],\n",
    "        \"cons\": [\"May be less creative\", \"Can be overly cautious\"]\n",
    "    },\n",
    "    \"Specialized Models\": {\n",
    "        \"description\": \"Trained for specific tasks or domains\",\n",
    "        \"examples\": [\"bert-base-uncased\", \"facebook/bart-large-cnn\"],\n",
    "        \"use_cases\": [\"Text classification\", \"Summarization\", \"Translation\"],\n",
    "        \"pros\": [\"Excellent at specific tasks\", \"Often smaller and faster\"],\n",
    "        \"cons\": [\"Limited to specific use cases\", \"Not for general chat\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_type, info in model_info.items():\n",
    "    print(f\"🏷️  {model_type}\")\n",
    "    print(f\"   📝 Description: {info['description']}\")\n",
    "    print(f\"   🔧 Examples: {', '.join(info['examples'])}\")\n",
    "    print(f\"   💼 Use cases: {', '.join(info['use_cases'])}\")\n",
    "    print(f\"   ✅ Pros: {', '.join(info['pros'])}\")\n",
    "    print(f\"   ⚠️  Cons: {', '.join(info['cons'])}\")\n",
    "    print()\n",
    "\n",
    "print(\"🎯 CHOOSING THE RIGHT MODEL:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"For beginners, start with:\")\n",
    "print(\"• distilgpt2 - Fastest, smallest GPT-2 variant\")\n",
    "print(\"• gpt2 - Classic text generation model\")\n",
    "print(\"• microsoft/DialoGPT-small - Conversational AI\")\n",
    "print()\n",
    "print(\"Consider these factors:\")\n",
    "print(\"• Model size (larger = better quality, slower speed)\")\n",
    "print(\"• Your hardware (RAM, GPU availability)\")\n",
    "print(\"• Intended use case (chat, generation, specific tasks)\")\n",
    "print(\"• Download time and storage space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Best Practices and Tips\n",
    "\n",
    "Here are some important best practices for working with local LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 BEST PRACTICES FOR LOCAL LLMS\n",
      "==================================================\n",
      "\n",
      "🚀 Performance Optimization:\n",
      "   • Use GPU when available (CUDA or Apple Silicon)\n",
      "   • Use torch.float16 for faster inference on compatible hardware\n",
      "   • Consider model quantization for memory efficiency\n",
      "   • Batch multiple requests when possible\n",
      "   • Use torch.no_grad() for inference to save memory\n",
      "\n",
      "💾 Memory Management:\n",
      "   • Monitor system memory usage, especially with large models\n",
      "   • Use device_map='auto' for automatic GPU memory distribution\n",
      "   • Clear model cache when switching between models\n",
      "   • Consider using smaller models for development/testing\n",
      "   • Use low_cpu_mem_usage=True when loading models\n",
      "\n",
      "🔧 Generation Quality:\n",
      "   • Experiment with temperature settings (0.1-1.2)\n",
      "   • Use appropriate max_length to avoid truncation\n",
      "   • Add proper context and prompting for better results\n",
      "   • Consider using top_k and top_p sampling\n",
      "   • Set appropriate stopping criteria\n",
      "\n",
      "🛡️ Safety and Ethics:\n",
      "   • Implement content filtering for production use\n",
      "   • Be aware of potential biases in model outputs\n",
      "   • Don't rely on model outputs for factual information\n",
      "   • Consider privacy implications of local vs cloud models\n",
      "   • Monitor and log model usage for debugging\n",
      "\n",
      "📚 Development Workflow:\n",
      "   • Start with small models for prototyping\n",
      "   • Test thoroughly before deploying larger models\n",
      "   • Version control your model configurations\n",
      "   • Document your prompting strategies\n",
      "   • Keep track of model versions and performance metrics\n",
      "\n",
      "📊 CURRENT RESOURCE USAGE:\n",
      "------------------------------\n",
      "RAM usage: 74.7% (9.7GB / 25.8GB)\n",
      "\n",
      "💡 Pro tip: The model 'distilgpt2' is using approximately:\n",
      "   • 0.2GB of storage space\n",
      "   • 0.3GB of RAM when loaded (float32)\n",
      "   • 0.2GB of RAM when loaded (float16)\n"
     ]
    }
   ],
   "source": [
    "# Best practices and tips\n",
    "print(\"💡 BEST PRACTICES FOR LOCAL LLMS\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "tips = {\n",
    "    \"🚀 Performance Optimization\": [\n",
    "        \"Use GPU when available (CUDA or Apple Silicon)\",\n",
    "        \"Use torch.float16 for faster inference on compatible hardware\",\n",
    "        \"Consider model quantization for memory efficiency\",\n",
    "        \"Batch multiple requests when possible\",\n",
    "        \"Use torch.no_grad() for inference to save memory\"\n",
    "    ],\n",
    "    \"💾 Memory Management\": [\n",
    "        \"Monitor system memory usage, especially with large models\",\n",
    "        \"Use device_map='auto' for automatic GPU memory distribution\",\n",
    "        \"Clear model cache when switching between models\",\n",
    "        \"Consider using smaller models for development/testing\",\n",
    "        \"Use low_cpu_mem_usage=True when loading models\"\n",
    "    ],\n",
    "    \"🔧 Generation Quality\": [\n",
    "        \"Experiment with temperature settings (0.1-1.2)\",\n",
    "        \"Use appropriate max_length to avoid truncation\",\n",
    "        \"Add proper context and prompting for better results\",\n",
    "        \"Consider using top_k and top_p sampling\",\n",
    "        \"Set appropriate stopping criteria\"\n",
    "    ],\n",
    "    \"🛡️ Safety and Ethics\": [\n",
    "        \"Implement content filtering for production use\",\n",
    "        \"Be aware of potential biases in model outputs\",\n",
    "        \"Don't rely on model outputs for factual information\",\n",
    "        \"Consider privacy implications of local vs cloud models\",\n",
    "        \"Monitor and log model usage for debugging\"\n",
    "    ],\n",
    "    \"📚 Development Workflow\": [\n",
    "        \"Start with small models for prototyping\",\n",
    "        \"Test thoroughly before deploying larger models\",\n",
    "        \"Version control your model configurations\",\n",
    "        \"Document your prompting strategies\",\n",
    "        \"Keep track of model versions and performance metrics\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, tip_list in tips.items():\n",
    "    print(f\"{category}:\")\n",
    "    for tip in tip_list:\n",
    "        print(f\"   • {tip}\")\n",
    "    print()\n",
    "\n",
    "# Resource usage check\n",
    "print(\"📊 CURRENT RESOURCE USAGE:\")\n",
    "print(\"-\" * 30)\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"RAM usage: {memory.percent}% ({memory.used / 1e9:.1f}GB / {memory.total / 1e9:.1f}GB)\")\n",
    "\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU memory: {gpu_memory:.1f}GB / {gpu_total:.1f}GB\")\n",
    "elif device == \"cpu\":\n",
    "    print(\"Running on CPU (no GPU memory usage)\")\n",
    "\n",
    "print(f\"\\n💡 Pro tip: The model '{MODEL_NAME}' is using approximately:\")\n",
    "print(f\"   • {model.num_parameters() * 2 / 1e9:.1f}GB of storage space\")\n",
    "print(f\"   • {model.num_parameters() * 4 / 1e9:.1f}GB of RAM when loaded (float32)\")\n",
    "print(f\"   • {model.num_parameters() * 2 / 1e9:.1f}GB of RAM when loaded (float16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully completed your first journey into local LLMs. Here's what you've accomplished:\n",
    "\n",
    "### ✅ What you learned:\n",
    "1. **Core Components**: Understanding LLMs, tokenizers, and generation parameters\n",
    "2. **Model Loading**: How to download and load models from HuggingFace\n",
    "3. **Text Generation**: Creating text with different parameters and approaches\n",
    "4. **Tokenization**: How text is converted to numbers and back\n",
    "5. **Performance**: Measuring speed and resource usage\n",
    "6. **Best Practices**: Optimization and safety considerations\n",
    "\n",
    "### 🔑 Key concepts mastered:\n",
    "- **Local vs Cloud**: Privacy, control, and offline capabilities\n",
    "- **Model Types**: Causal LMs, instruction-tuned, and specialized models\n",
    "- **Generation Parameters**: Temperature, sampling, max length\n",
    "- **Resource Management**: Memory, GPU usage, performance optimization\n",
    "\n",
    "### 🚀 Suggested next steps:\n",
    "\n",
    "#### Immediate experiments:\n",
    "1. **Try different models**: Load `gpt2`, `microsoft/DialoGPT-small`, or `facebook/opt-125m`\n",
    "2. **Experiment with parameters**: Test different temperature and sampling settings\n",
    "3. **Create longer conversations**: Build a chat interface with memory\n",
    "4. **Specialized tasks**: Try summarization, question-answering, or creative writing\n",
    "\n",
    "#### Advanced projects:\n",
    "1. **Fine-tuning**: Customize models for your specific use case\n",
    "2. **RAG (Retrieval-Augmented Generation)**: Combine models with external knowledge\n",
    "3. **Model optimization**: Quantization, pruning, and distillation\n",
    "4. **Production deployment**: API servers, containerization, scaling\n",
    "\n",
    "#### Learning resources:\n",
    "- [HuggingFace Transformers Documentation](https://huggingface.co/docs/transformers)\n",
    "- [HuggingFace Model Hub](https://huggingface.co/models)\n",
    "- [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "### 💡 Remember:\n",
    "- Start small and iterate\n",
    "- Local models give you privacy and control\n",
    "- Experiment with different models for different tasks\n",
    "- Monitor resource usage and optimize accordingly\n",
    "- Always consider safety and ethical implications\n",
    "\n",
    "### 🔄 Compare with other approaches:\n",
    "You can also try the Ollama approach for an even simpler setup:\n",
    "- Check out `getting_started_ollama.ipynb` for a different approach\n",
    "- Ollama provides pre-optimized models with simple API\n",
    "- Both approaches have their advantages depending on your use case\n",
    "\n",
    "Happy experimenting with your local LLMs! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
