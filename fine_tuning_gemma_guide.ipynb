{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Gemma 3 1B Instruct: Complete Guide 🎯\n",
    "\n",
    "Welcome to the comprehensive guide for fine-tuning Google's Gemma 3 1B Instruct model! This notebook will walk you through the entire process of customizing a pre-trained language model for your specific use case.\n",
    "\n",
    "## What you'll learn:\n",
    "- Understanding fine-tuning vs training from scratch\n",
    "- Setting up the environment for different devices (CPU, CUDA, Apple Silicon)\n",
    "- Loading and preparing the Gemma 3 1B Instruct model\n",
    "- Creating and formatting training datasets\n",
    "- Implementing LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "- Training with different optimization techniques\n",
    "- Evaluating and testing your fine-tuned model\n",
    "- Saving and sharing your custom model\n",
    "\n",
    "## Prerequisites:\n",
    "- Python 3.8 or higher\n",
    "- At least 16GB of RAM (32GB+ recommended)\n",
    "- GPU with 8GB+ VRAM (or Apple Silicon with 16GB+ unified memory)\n",
    "- HuggingFace account and token for Gemma access\n",
    "- Basic understanding of machine learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Fine-Tuning\n",
    "\n",
    "Before we start, let's understand what fine-tuning means and why it's powerful:\n",
    "\n",
    "### 🧠 **What is Fine-Tuning?**\n",
    "Fine-tuning takes a pre-trained model and adapts it to your specific task or domain by training it on your custom dataset.\n",
    "\n",
    "### 🎯 **Types of Fine-Tuning:**\n",
    "- **Full Fine-Tuning**: Updates all model parameters (expensive, high quality)\n",
    "- **LoRA (Low-Rank Adaptation)**: Updates only small adapter layers (efficient, good quality)\n",
    "- **Prompt Tuning**: Learns optimal prompts (very efficient, task-specific)\n",
    "\n",
    "### 💡 **Why Fine-Tune Gemma 1B?**\n",
    "- Smaller model = faster training and inference\n",
    "- Good performance for many tasks\n",
    "- Fits in consumer hardware\n",
    "- Active community and support\n",
    "\n",
    "### 📊 **Device Considerations:**\n",
    "- **Apple Silicon (M1/M2/M3)**: Great for LoRA fine-tuning, unified memory advantage\n",
    "- **NVIDIA GPUs**: Excellent for all types of fine-tuning\n",
    "- **CPU Only**: Possible but slow, best for very small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Required Libraries\n",
    "\n",
    "Let's install all the necessary libraries for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing fine-tuning packages...\n",
      "⚠️  This may take several minutes\n",
      "\n",
      "Installing transformers>=4.36.0...\n",
      "Requirement already satisfied: transformers>=4.36.0 in ./myenv/lib/python3.12/site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (2025.9.18)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./myenv/lib/python3.12/site-packages (from transformers>=4.36.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->transformers>=4.36.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->transformers>=4.36.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->transformers>=4.36.0) (2025.8.3)\n",
      "✅ transformers>=4.36.0 installed successfully\n",
      "Installing torch>=2.1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.1.0 in ./myenv/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib/python3.12/site-packages (from torch>=2.1.0) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
      "✅ torch>=2.1.0 installed successfully\n",
      "Installing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./myenv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./myenv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in ./myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./myenv/lib/python3.12/site-packages (from datasets) (0.35.0)\n",
      "Requirement already satisfied: packaging in ./myenv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohttp-3.12.15-cp312-cp312-macosx_11_0_arm64.whl (469 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.7.0-cp312-cp312-macosx_11_0_arm64.whl (46 kB)\n",
      "Downloading multidict-6.6.4-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached propcache-0.3.2-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached yarl-1.20.1-cp312-cp312-macosx_11_0_arm64.whl (89 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 datasets-4.1.1 dill-0.4.0 frozenlist-1.7.0 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ datasets installed successfully\n",
      "Installing accelerate...\n",
      "Requirement already satisfied: accelerate in ./myenv/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./myenv/lib/python3.12/site-packages (from accelerate) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./myenv/lib/python3.12/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in ./myenv/lib/python3.12/site-packages (from accelerate) (0.35.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./myenv/lib/python3.12/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
      "✅ accelerate installed successfully\n",
      "Installing peft...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.12/site-packages (from peft) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (from peft) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.12/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./myenv/lib/python3.12/site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: transformers in ./myenv/lib/python3.12/site-packages (from peft) (4.56.1)\n",
      "Requirement already satisfied: tqdm in ./myenv/lib/python3.12/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./myenv/lib/python3.12/site-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in ./myenv/lib/python3.12/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in ./myenv/lib/python3.12/site-packages (from peft) (0.35.0)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2025.9.0)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.10)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.12/site-packages (from transformers->peft) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./myenv/lib/python3.12/site-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n",
      "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.17.1\n",
      "✅ peft installed successfully\n",
      "Installing bitsandbytes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting scipy (from bitsandbytes)\n",
      "  Downloading scipy-1.16.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in ./myenv/lib/python3.12/site-packages (from scipy->bitsandbytes) (2.3.3)\n",
      "Using cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "Downloading scipy-1.16.2-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.42.0 scipy-1.16.2\n",
      "✅ bitsandbytes installed successfully\n",
      "Installing trl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in ./myenv/lib/python3.12/site-packages (from trl) (1.10.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in ./myenv/lib/python3.12/site-packages (from trl) (4.1.1)\n",
      "Requirement already satisfied: transformers>=4.56.1 in ./myenv/lib/python3.12/site-packages (from trl) (4.56.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (25.0)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (0.35.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./myenv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./myenv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in ./myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.12/site-packages (from transformers>=4.56.1->trl) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./myenv/lib/python3.12/site-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
      "Downloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trl\n",
      "Successfully installed trl-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ trl installed successfully\n",
      "Installing wandb...\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.22.0-py3-none-macosx_12_0_arm64.whl.metadata (10 kB)\n",
      "Collecting click>=8.0.1 (from wandb)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in ./myenv/lib/python3.12/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in ./myenv/lib/python3.12/site-packages (from wandb) (4.4.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Using cached pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./myenv/lib/python3.12/site-packages (from wandb) (2.32.5)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.38.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in ./myenv/lib/python3.12/site-packages (from wandb) (4.15.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.22.0-py3-none-macosx_12_0_arm64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
      "Using cached pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Downloading sentry_sdk-2.38.0-py2.py3-none-any.whl (370 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: typing-inspection, smmap, sentry-sdk, pydantic-core, protobuf, click, annotated-types, pydantic, gitdb, gitpython, wandb\n",
      "Successfully installed annotated-types-0.7.0 click-8.3.0 gitdb-4.0.12 gitpython-3.1.45 protobuf-6.32.1 pydantic-2.11.9 pydantic-core-2.33.2 sentry-sdk-2.38.0 smmap-5.0.2 typing-inspection-0.4.1 wandb-0.22.0\n",
      "✅ wandb installed successfully\n",
      "Installing psutil...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (7.1.0)\n",
      "✅ psutil installed successfully\n",
      "Installing sentencepiece...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in ./myenv/lib/python3.12/site-packages (0.2.1)\n",
      "✅ sentencepiece installed successfully\n",
      "Installing protobuf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in ./myenv/lib/python3.12/site-packages (6.32.1)\n",
      "✅ protobuf installed successfully\n",
      "\n",
      "🎉 Installation complete!\n",
      "\n",
      "💡 Note: Some packages may show warnings - this is normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries for fine-tuning\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Core libraries for fine-tuning\n",
    "packages = [\n",
    "    \"transformers>=4.36.0\",    # Latest transformers with Gemma support\n",
    "    \"torch>=2.1.0\",           # PyTorch with MPS support\n",
    "    \"datasets\",               # For dataset handling\n",
    "    \"accelerate\",             # For distributed training\n",
    "    \"peft\",                   # For LoRA and other parameter-efficient methods\n",
    "    \"bitsandbytes\",           # For quantization (if supported)\n",
    "    \"trl\",                    # For training utilities\n",
    "    \"wandb\",                  # For experiment tracking (optional)\n",
    "    \"psutil\",                 # For system monitoring\n",
    "    \"sentencepiece\",          # For tokenization\n",
    "    \"protobuf\",               # Required for some tokenizers\n",
    "]\n",
    "\n",
    "print(\"📦 Installing fine-tuning packages...\")\n",
    "print(\"⚠️  This may take several minutes\")\n",
    "print()\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        install_package(package)\n",
    "        print(f\"✅ {package} installed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to install {package}: {e}\")\n",
    "        if \"bitsandbytes\" in package:\n",
    "            print(\"💡 bitsandbytes may not be available on Apple Silicon - this is OK\")\n",
    "\n",
    "print(\"\\n🎉 Installation complete!\")\n",
    "print(\"\\n💡 Note: Some packages may show warnings - this is normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Environment Setup and Device Detection\n",
    "\n",
    "Let's set up our environment and detect the best device for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️  SYSTEM INFORMATION\n",
      "============================================================\n",
      "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n",
      "PyTorch version: 2.8.0\n",
      "Transformers version: 4.56.1\n",
      "\n",
      "💾 MEMORY INFORMATION:\n",
      "Total RAM: 24.0 GB\n",
      "Available RAM: 5.9 GB\n",
      "RAM usage: 75.3%\n",
      "\n",
      "🚀 DEVICE DETECTION:\n",
      "✅ Apple Silicon (MPS) available\n",
      "   Unified memory: 24.0 GB\n",
      "   MPS is ideal for LoRA fine-tuning\n",
      "\n",
      "🎯 Selected device: mps\n",
      "\n",
      "📋 TRAINING RECOMMENDATIONS:\n",
      "   • LoRA fine-tuning recommended\n",
      "   • Batch size: 2-8 depending on memory\n",
      "   • Use float16 precision\n",
      "\n",
      "✅ Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import transformers  # Import the module itself to access __version__\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import psutil\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# System information\n",
    "print(\"🖥️  SYSTEM INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Memory information\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"\\n💾 MEMORY INFORMATION:\")\n",
    "print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "print(f\"RAM usage: {memory.percent:.1f}%\")\n",
    "\n",
    "# Device detection with detailed information\n",
    "print(f\"\\n🚀 DEVICE DETECTION:\")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"✅ CUDA available with {gpu_count} GPU(s)\")\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        print(f\"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(f\"✅ Apple Silicon (MPS) available\")\n",
    "    print(f\"   Unified memory: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"   MPS is ideal for LoRA fine-tuning\")\n",
    "    \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"⚠️  Using CPU only\")\n",
    "    print(f\"   Training will be slower but still possible\")\n",
    "    print(f\"   Consider using smaller batch sizes\")\n",
    "\n",
    "print(f\"\\n🎯 Selected device: {device}\")\n",
    "\n",
    "# Training recommendations based on device\n",
    "print(f\"\\n📋 TRAINING RECOMMENDATIONS:\")\n",
    "if device == \"cuda\":\n",
    "    print(\"   • Use LoRA or full fine-tuning\")\n",
    "    print(\"   • Batch size: 4-16 depending on GPU memory\")\n",
    "    print(\"   • Enable gradient checkpointing for larger models\")\n",
    "elif device == \"mps\":\n",
    "    print(\"   • LoRA fine-tuning recommended\")\n",
    "    print(\"   • Batch size: 2-8 depending on memory\")\n",
    "    print(\"   • Use float16 precision\")\n",
    "else:\n",
    "    print(\"   • LoRA fine-tuning only\")\n",
    "    print(\"   • Small batch size: 1-2\")\n",
    "    print(\"   • Consider using smaller dataset\")\n",
    "\n",
    "print(\"\\n✅ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: HuggingFace Authentication\n",
    "\n",
    "Gemma models require authentication. Let's set up your HuggingFace token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "print(f\"📥 Loading Gemma 3 1B Instruct model: {MODEL_NAME}\")\n",
    "print(\"⏳ This may take several minutes on first run...\")\n",
    "print()\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"🔤 Loading tokenizer...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Add padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"📝 Added padding token\")\n",
    "    \n",
    "    print(f\"✅ Tokenizer loaded successfully\")\n",
    "    print(f\"   Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"   Model max length: {tokenizer.model_max_length}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading tokenizer: {e}\")\n",
    "    print(\"💡 Make sure you have accepted the Gemma license and are logged in\")\n",
    "\n",
    "# Load model with device-specific configuration\n",
    "print(f\"\\n🧠 Loading model for {device}...\")\n",
    "\n",
    "# Device-specific model loading\n",
    "try:\n",
    "    if device == \"cuda\":\n",
    "        # CUDA configuration\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.bfloat16,  # Use bfloat16 for better performance\n",
    "            device_map=\"auto\",           # Automatic device mapping\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "    elif device == \"mps\":\n",
    "        # Apple Silicon configuration\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16,   # Use float16 for MPS\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        \n",
    "    else:\n",
    "        # CPU configuration\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float32,   # Use float32 for CPU\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = model.to(device)\n",
    "    \n",
    "    print(\"✅ Model loaded successfully\")\n",
    "    \n",
    "    # Model information\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n📊 MODEL INFORMATION:\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   Model size: ~{total_params * 2 / 1e9:.1f} GB (float16)\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if device == \"cuda\":\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"   GPU memory used: {gpu_memory:.1f} GB\")\n",
    "    elif device == \"mps\":\n",
    "        print(f\"   Running on Apple Silicon MPS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"💡 Possible solutions:\")\n",
    "    print(\"   • Check your HuggingFace authentication\")\n",
    "    print(\"   • Ensure you have enough memory\")\n",
    "    print(\"   • Try reducing precision or using CPU\")\n",
    "    print(\"   • Make sure you have access to the Gemma 3 model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load and Prepare the Gemma 1B Model\n",
    "\n",
    "Now let's load the Gemma 1B model with proper configuration for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading Gemma 1B model: google/gemma-3-1b-it\n",
      "⏳ This may take several minutes on first run...\n",
      "\n",
      "🔤 Loading tokenizer...\n",
      "✅ Tokenizer loaded successfully\n",
      "   Vocabulary size: 262145\n",
      "   Model max length: 1000000000000000019884624838656\n",
      "\n",
      "🧠 Loading model for mps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully\n",
      "\n",
      "📊 MODEL INFORMATION:\n",
      "   Total parameters: 999,885,952\n",
      "   Trainable parameters: 999,885,952\n",
      "   Model size: ~2.0 GB (float16)\n",
      "   Running on Apple Silicon MPS\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "print(f\"📥 Loading Gemma 1B model: {MODEL_NAME}\")\n",
    "print(\"⏳ This may take several minutes on first run...\")\n",
    "print()\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"🔤 Loading tokenizer...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Add padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"📝 Added padding token\")\n",
    "    \n",
    "    print(f\"✅ Tokenizer loaded successfully\")\n",
    "    print(f\"   Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"   Model max length: {tokenizer.model_max_length}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading tokenizer: {e}\")\n",
    "    print(\"💡 Make sure you have accepted the Gemma license and are logged in\")\n",
    "\n",
    "# Load model with device-specific configuration\n",
    "print(f\"\\n🧠 Loading model for {device}...\")\n",
    "\n",
    "# Device-specific model loading\n",
    "try:\n",
    "    if device == \"cuda\":\n",
    "        # CUDA configuration\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.bfloat16,  # Use bfloat16 for better performance\n",
    "            device_map=\"auto\",           # Automatic device mapping\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "    elif device == \"mps\":\n",
    "        # Apple Silicon configuration\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16,   # Use float16 for MPS\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        \n",
    "    else:\n",
    "        # CPU configuration\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float32,   # Use float32 for CPU\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = model.to(device)\n",
    "    \n",
    "    print(\"✅ Model loaded successfully\")\n",
    "    \n",
    "    # Model information\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n📊 MODEL INFORMATION:\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   Model size: ~{total_params * 2 / 1e9:.1f} GB (float16)\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if device == \"cuda\":\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"   GPU memory used: {gpu_memory:.1f} GB\")\n",
    "    elif device == \"mps\":\n",
    "        print(f\"   Running on Apple Silicon MPS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"💡 Possible solutions:\")\n",
    "    print(\"   • Check your HuggingFace authentication\")\n",
    "    print(\"   • Ensure you have enough memory\")\n",
    "    print(\"   • Try reducing precision or using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prepare Training Dataset\n",
    "\n",
    "Let's create a sample dataset for fine-tuning. You can replace this with your own data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 PREPARING TRAINING DATASET\n",
      "==================================================\n",
      "📝 Created dataset with 100 examples\n",
      "\n",
      "📋 Example formatted training sample:\n",
      "----------------------------------------\n",
      "### Instruction:\n",
      "Explain what machine learning is in simple terms.\n",
      "\n",
      "### Response:\n",
      "Machine learning is a type of artificial intelligence where computers learn to make predictions or decisions by finding patterns in data, rather than being explicitly programmed for every task.\n",
      "----------------------------------------\n",
      "\n",
      "✅ Dataset created with 100 examples\n",
      "   Example keys: ['text']\n",
      "   Training examples: 80\n",
      "   Validation examples: 20\n",
      "\n",
      "💡 Note: In practice, you should use a much larger dataset (1000+ examples)\n",
      "   for better fine-tuning results. This is just a demonstration.\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataset for demonstration\n",
    "# Replace this with your own dataset!\n",
    "\n",
    "print(\"📚 PREPARING TRAINING DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample data: AI assistant training examples\n",
    "sample_data = [\n",
    "    {\n",
    "        \"instruction\": \"Explain what machine learning is in simple terms.\",\n",
    "        \"response\": \"Machine learning is a type of artificial intelligence where computers learn to make predictions or decisions by finding patterns in data, rather than being explicitly programmed for every task.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the benefits of renewable energy?\",\n",
    "        \"response\": \"Renewable energy sources like solar and wind power are sustainable, reduce greenhouse gas emissions, decrease dependence on fossil fuels, and can lead to long-term cost savings while creating jobs in green technology sectors.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How do you make a simple vegetable soup?\",\n",
    "        \"response\": \"To make vegetable soup: 1) Heat oil in a pot, 2) Sauté diced onions and garlic, 3) Add chopped vegetables like carrots, celery, and potatoes, 4) Pour in vegetable broth, 5) Season with herbs and spices, 6) Simmer until vegetables are tender, about 20-30 minutes.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the importance of exercise for mental health?\",\n",
    "        \"response\": \"Exercise significantly benefits mental health by releasing endorphins that improve mood, reducing stress and anxiety, improving sleep quality, boosting self-esteem, and providing a healthy outlet for managing emotions and daily pressures.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of photosynthesis.\",\n",
    "        \"response\": \"Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose (sugar) and oxygen. This occurs mainly in plant leaves using chlorophyll, providing energy for the plant and producing oxygen as a byproduct that's essential for most life on Earth.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extend the dataset by repeating and varying the examples\n",
    "# In practice, you'd want hundreds or thousands of examples\n",
    "extended_data = []\n",
    "for i in range(20):  # Repeat each example multiple times\n",
    "    for item in sample_data:\n",
    "        extended_data.append(item)\n",
    "\n",
    "print(f\"📝 Created dataset with {len(extended_data)} examples\")\n",
    "\n",
    "# Format data for instruction following\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format the data as instruction-following examples\"\"\"\n",
    "    return f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n",
    "\n",
    "# Apply formatting\n",
    "formatted_texts = [format_instruction(item) for item in extended_data]\n",
    "\n",
    "print(\"\\n📋 Example formatted training sample:\")\n",
    "print(\"-\" * 40)\n",
    "print(formatted_texts[0])\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "dataset = Dataset.from_dict({\"text\": formatted_texts})\n",
    "\n",
    "print(f\"\\n✅ Dataset created with {len(dataset)} examples\")\n",
    "print(f\"   Example keys: {list(dataset.features.keys())}\")\n",
    "\n",
    "# Split into train/validation\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "print(f\"   Training examples: {len(train_dataset)}\")\n",
    "print(f\"   Validation examples: {len(eval_dataset)}\")\n",
    "\n",
    "print(\"\\n💡 Note: In practice, you should use a much larger dataset (1000+ examples)\")\n",
    "print(\"   for better fine-tuning results. This is just a demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Tokenize the Dataset\n",
    "\n",
    "Now let's tokenize our dataset for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 TOKENIZING DATASET\n",
      "========================================\n",
      "Max sequence length: 512\n",
      "🔄 Tokenizing training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing training data: 100%|██████████| 80/80 [00:00<00:00, 4127.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Tokenizing validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data: 100%|██████████| 20/20 [00:00<00:00, 5069.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenization complete!\n",
      "\n",
      "📊 TOKENIZATION STATISTICS:\n",
      "   Sample input_ids length: 512\n",
      "   Sample attention_mask length: 512\n",
      "   Number of padding tokens in sample: 442\n",
      "\n",
      "🔍 SAMPLE TOKENIZED TEXT:\n",
      "First 50 tokens decoded: ...\n",
      "\n",
      "💾 Dataset ready for training!\n",
      "   Training samples: 80\n",
      "   Validation samples: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization configuration\n",
    "MAX_LENGTH = 512  # Adjust based on your data and memory\n",
    "\n",
    "print(f\"🔤 TOKENIZING DATASET\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text examples\"\"\"\n",
    "    # Tokenize the texts\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"🔄 Tokenizing training dataset...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing training data\"\n",
    ")\n",
    "\n",
    "print(\"🔄 Tokenizing validation dataset...\")\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    desc=\"Tokenizing validation data\"\n",
    ")\n",
    "\n",
    "print(\"✅ Tokenization complete!\")\n",
    "\n",
    "# Examine tokenized data\n",
    "sample_tokens = tokenized_train[0]\n",
    "print(f\"\\n📊 TOKENIZATION STATISTICS:\")\n",
    "print(f\"   Sample input_ids length: {len(sample_tokens['input_ids'])}\")\n",
    "print(f\"   Sample attention_mask length: {len(sample_tokens['attention_mask'])}\")\n",
    "print(f\"   Number of padding tokens in sample: {sample_tokens['attention_mask'].count(0)}\")\n",
    "\n",
    "# Show a sample of tokenized text\n",
    "print(f\"\\n🔍 SAMPLE TOKENIZED TEXT:\")\n",
    "sample_text = tokenizer.decode(sample_tokens['input_ids'][:50], skip_special_tokens=True)\n",
    "print(f\"First 50 tokens decoded: {sample_text}...\")\n",
    "\n",
    "print(f\"\\n💾 Dataset ready for training!\")\n",
    "print(f\"   Training samples: {len(tokenized_train)}\")\n",
    "print(f\"   Validation samples: {len(tokenized_eval)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Setup LoRA Configuration\n",
    "\n",
    "We'll use LoRA (Low-Rank Adaptation) for efficient fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 SETTING UP LORA CONFIGURATION\n",
      "==================================================\n",
      "📋 LoRA Configuration:\n",
      "   Rank (r): 16\n",
      "   Alpha: 32\n",
      "   Dropout: 0.1\n",
      "   Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "\n",
      "🔄 Applying LoRA to model...\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "✅ LoRA applied successfully!\n",
      "trainable params: 13,045,760 || all params: 1,012,931,712 || trainable%: 1.2879\n",
      "\n",
      "📊 PARAMETER EFFICIENCY:\n",
      "   Total parameters: 1,012,931,712\n",
      "   Trainable parameters: 13,045,760\n",
      "   Percentage trainable: 1.29%\n",
      "   Memory reduction: ~98.7%\n",
      "\n",
      "🎯 LoRA setup complete! Ready for efficient fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration for efficient fine-tuning\n",
    "print(\"🔧 SETTING UP LORA CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# LoRA parameters\n",
    "LORA_R = 16        # Rank of adaptation\n",
    "LORA_ALPHA = 32    # LoRA scaling parameter\n",
    "LORA_DROPOUT = 0.1 # LoRA dropout\n",
    "\n",
    "# Define target modules for LoRA (Gemma-specific)\n",
    "target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\", \n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"\n",
    "]\n",
    "\n",
    "# Create LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "print(f\"📋 LoRA Configuration:\")\n",
    "print(f\"   Rank (r): {LORA_R}\")\n",
    "print(f\"   Alpha: {LORA_ALPHA}\")\n",
    "print(f\"   Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"   Target modules: {target_modules}\")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "print(f\"\\n🔄 Applying LoRA to model...\")\n",
    "try:\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(\"✅ LoRA applied successfully!\")\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Calculate parameter efficiency\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n📊 PARAMETER EFFICIENCY:\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "    print(f\"   Memory reduction: ~{(total_params - trainable_params) / total_params * 100:.1f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error applying LoRA: {e}\")\n",
    "    print(\"💡 This might be due to model architecture or memory issues\")\n",
    "\n",
    "print(f\"\\n🎯 LoRA setup complete! Ready for efficient fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Configure Training Arguments\n",
    "\n",
    "Let's set up training parameters optimized for different devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SAVING AND PREPARING MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Create model card with training information\u001b[39;00m\n\u001b[32m      6\u001b[39m model_card_content = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m# Gemma 3 1B Instruct Fine-tuned Model\u001b[39m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33m## Model Description\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33mThis is a fine-tuned version of Google\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms Gemma 3 1B Instruct model, adapted for custom instruction-following tasks.\u001b[39m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[33m## Training Details\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m- **Base model**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33m- **Fine-tuning method**: LoRA (Low-Rank Adaptation)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m- **Training device**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33m- **LoRA rank**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_R\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33m- **LoRA alpha**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_ALPHA\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[33m- **Training epochs**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtraining_args\u001b[49m.num_train_epochs\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33m- **Learning rate**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args.learning_rate\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33m- **Batch size**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meffective_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (effective)\u001b[39m\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m \u001b[33m## Usage\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[33m```python\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[33mfrom transformers import AutoTokenizer, AutoModelForCausalLM\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[33mfrom peft import PeftModel\u001b[39m\n\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[33m# Load tokenizer and base model\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[33mtokenizer = AutoTokenizer.from_pretrained(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgoogle/gemma-3-1b-it\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[33mbase_model = AutoModelForCausalLM.from_pretrained(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgoogle/gemma-3-1b-it\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[33m# Load fine-tuned model\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[33mmodel = PeftModel.from_pretrained(base_model, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpath/to/this/model\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[33m# Generate text\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[33mprompt = \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m### Instruction:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mnYour question here\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn### Response:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[33minputs = tokenizer.encode(prompt, return_tensors=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[33moutputs = model.generate(inputs, max_length=200, temperature=0.7)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[33mresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[33m```\u001b[39m\n\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m \u001b[33m## Training Data\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[33mThe model was fine-tuned on a custom instruction-following dataset.\u001b[39m\n\u001b[32m     43\u001b[39m \n\u001b[32m     44\u001b[39m \u001b[33m## Limitations\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[33m- This is a demonstration model with limited training data\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[33m- May not generalize well to all tasks\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[33m- Requires the same format for optimal performance\u001b[39m\n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[33m## License\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[33mThis model inherits the license from the base Gemma 3 model.\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Save model card\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/README.md\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'training_args' is not defined"
     ]
    }
   ],
   "source": [
    "# Save and prepare model for sharing\n",
    "print(\"💾 SAVING AND PREPARING MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create model card with training information\n",
    "model_card_content = f\"\"\"\n",
    "# Gemma 3 1B Instruct Fine-tuned Model\n",
    "\n",
    "## Model Description\n",
    "This is a fine-tuned version of Google's Gemma 3 1B Instruct model, adapted for custom instruction-following tasks.\n",
    "\n",
    "## Training Details\n",
    "- **Base model**: {MODEL_NAME}\n",
    "- **Fine-tuning method**: LoRA (Low-Rank Adaptation)\n",
    "- **Training device**: {device}\n",
    "- **LoRA rank**: {LORA_R}\n",
    "- **LoRA alpha**: {LORA_ALPHA}\n",
    "- **Training epochs**: {training_args.num_train_epochs}\n",
    "- **Learning rate**: {training_args.learning_rate}\n",
    "- **Batch size**: {effective_batch_size} (effective)\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/this/model\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"### Instruction:\\\\\\\\nYour question here\\\\\\\\n\\\\\\\\n### Response:\\\\\\\\n\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=200, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "## Training Data\n",
    "The model was fine-tuned on a custom instruction-following dataset.\n",
    "\n",
    "## Limitations\n",
    "- This is a demonstration model with limited training data\n",
    "- May not generalize well to all tasks\n",
    "- Requires the same format for optimal performance\n",
    "\n",
    "## License\n",
    "This model inherits the license from the base Gemma 3 model.\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(f\"{output_dir}/README.md\", \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "\n",
    "print(\"📝 Model card created\")\n",
    "\n",
    "# Save training configuration\n",
    "training_config = {\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"device\": device,\n",
    "    \"lora_config\": {\n",
    "        \"r\": LORA_R,\n",
    "        \"alpha\": LORA_ALPHA,\n",
    "        \"dropout\": LORA_DROPOUT,\n",
    "        \"target_modules\": target_modules\n",
    "    },\n",
    "    \"training_args\": training_args.to_dict() if hasattr(training_args, 'to_dict') else str(training_args),\n",
    "    \"dataset_size\": len(extended_data),\n",
    "    \"max_length\": MAX_LENGTH\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2, default=str)\n",
    "\n",
    "print(\"⚙️ Training configuration saved\")\n",
    "\n",
    "# List all saved files\n",
    "print(f\"\\n📁 SAVED FILES IN {output_dir}:\")\n",
    "try:\n",
    "    for file in os.listdir(output_dir):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"   {file} ({size_mb:.1f} MB)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error listing files: {e}\")\n",
    "\n",
    "# Instructions for using the model\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(f\"1. Test the model thoroughly with your use case\")\n",
    "print(f\"2. If performance is good, consider training with more data\")\n",
    "print(f\"3. You can push to HuggingFace Hub for sharing:\")\n",
    "print(f\"   model.push_to_hub('your-username/gemma-3-1b-it-finetuned')\")\n",
    "print(f\"4. Or share the '{output_dir}' folder directly\")\n",
    "\n",
    "print(f\"\\n✅ Model preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Setup Data Collator and Trainer\n",
    "\n",
    "Now let's set up the data collator and trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data collator\n",
    "print(\"📦 SETTING UP DATA COLLATOR AND TRAINER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM\n",
    "    pad_to_multiple_of=8,  # For efficiency\n",
    ")\n",
    "\n",
    "print(\"✅ Data collator created\")\n",
    "\n",
    "# Create trainer\n",
    "print(\"🏋️ Creating trainer...\")\n",
    "\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Trainer created successfully!\")\n",
    "    \n",
    "    # Print training setup summary\n",
    "    print(f\"\\n📊 TRAINING SETUP SUMMARY:\")\n",
    "    print(f\"   Model: {MODEL_NAME}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Training method: LoRA fine-tuning\")\n",
    "    print(f\"   Training samples: {len(tokenized_train)}\")\n",
    "    print(f\"   Validation samples: {len(tokenized_eval)}\")\n",
    "    print(f\"   Output directory: {output_dir}\")\n",
    "    \n",
    "    # Memory check before training\n",
    "    if device == \"cuda\":\n",
    "        memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"   GPU memory: {memory_used:.1f}GB / {memory_total:.1f}GB\")\n",
    "    elif device == \"mps\":\n",
    "        print(f\"   Apple Silicon unified memory in use\")\n",
    "    \n",
    "    current_memory = psutil.virtual_memory()\n",
    "    print(f\"   System RAM: {current_memory.percent:.1f}% used\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating trainer: {e}\")\n",
    "    print(\"💡 This might be due to memory or configuration issues\")\n",
    "\n",
    "print(f\"\\n🎯 Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Start Fine-Tuning\n",
    "\n",
    "Now let's start the actual fine-tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"🚀 STARTING FINE-TUNING PROCESS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"⚠️  This may take some time depending on your hardware\")\n",
    "print(\"💡 Monitor GPU/CPU usage and temperature\")\n",
    "print()\n",
    "\n",
    "import time\n",
    "\n",
    "# Record training start time\n",
    "training_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    print(\"🏋️ Beginning training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_end_time = time.time()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    \n",
    "    print(f\"\\n🎉 TRAINING COMPLETED!\")\n",
    "    print(f\"   Total training time: {training_duration / 60:.1f} minutes\")\n",
    "    print(f\"   Average time per epoch: {training_duration / training_args.num_train_epochs / 60:.1f} minutes\")\n",
    "    \n",
    "    # Get training metrics\n",
    "    train_metrics = trainer.state.log_history\n",
    "    if train_metrics:\n",
    "        final_train_loss = None\n",
    "        final_eval_loss = None\n",
    "        \n",
    "        # Find the final losses\n",
    "        for log in reversed(train_metrics):\n",
    "            if 'train_loss' in log and final_train_loss is None:\n",
    "                final_train_loss = log['train_loss']\n",
    "            if 'eval_loss' in log and final_eval_loss is None:\n",
    "                final_eval_loss = log['eval_loss']\n",
    "            if final_train_loss is not None and final_eval_loss is not None:\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n📊 FINAL METRICS:\")\n",
    "        if final_train_loss:\n",
    "            print(f\"   Final training loss: {final_train_loss:.4f}\")\n",
    "        if final_eval_loss:\n",
    "            print(f\"   Final validation loss: {final_eval_loss:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    print(f\"\\n💾 Saving fine-tuned model...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"✅ Model saved to: {output_dir}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n⏹️  Training interrupted by user\")\n",
    "    print(f\"   Partial model may be saved in {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training failed: {e}\")\n",
    "    print(f\"💡 Common issues:\")\n",
    "    print(f\"   • Out of memory: Reduce batch size or use gradient checkpointing\")\n",
    "    print(f\"   • Model too large: Use smaller model or more aggressive LoRA settings\")\n",
    "    print(f\"   • Device issues: Check CUDA/MPS availability\")\n",
    "    \n",
    "    # Try to save partial progress\n",
    "    try:\n",
    "        trainer.save_model(output_dir + \"_partial\")\n",
    "        print(f\"📁 Partial model saved to: {output_dir}_partial\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\n🏁 Training session complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test the Fine-Tuned Model\n",
    "\n",
    "Let's test our fine-tuned model to see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "print(\"🧪 TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the fine-tuned model if needed\n",
    "if 'trainer' in locals() and trainer.model is not None:\n",
    "    fine_tuned_model = trainer.model\n",
    "    print(\"✅ Using model from training session\")\n",
    "else:\n",
    "    # Load from saved checkpoint\n",
    "    print(\"📥 Loading fine-tuned model from disk...\")\n",
    "    try:\n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load LoRA weights\n",
    "        fine_tuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "        fine_tuned_model = fine_tuned_model.to(device)\n",
    "        print(\"✅ Fine-tuned model loaded successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading fine-tuned model: {e}\")\n",
    "        print(\"💡 Using original model for comparison\")\n",
    "        fine_tuned_model = model\n",
    "\n",
    "# Set model to evaluation mode\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"### Instruction:\\nExplain what deep learning is in simple terms.\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nWhat are the benefits of eating vegetables?\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nHow do you learn a new programming language?\\n\\n### Response:\\n\"\n",
    "]\n",
    "\n",
    "print(\"\\n🔍 TESTING WITH SAMPLE PROMPTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n📝 TEST {i}:\")\n",
    "    print(f\"Prompt: {prompt.split('### Response:')[0].split('### Instruction:')[1].strip()}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = fine_tuned_model.generate(\n",
    "                inputs,\n",
    "                max_length=inputs.shape[1] + 150,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_only = full_response[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"Response: {response_only}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating response: {e}\")\n",
    "\n",
    "print(\"\\n📊 EVALUATION NOTES:\")\n",
    "print(\"• Compare responses to the original model behavior\")\n",
    "print(\"• Look for improved instruction following\")\n",
    "print(\"• Check if responses are more relevant to the format\")\n",
    "print(\"• With more training data and epochs, quality should improve\")\n",
    "\n",
    "print(\"\\n✅ Model testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save and Share Your Model\n",
    "\n",
    "Let's prepare the model for sharing and future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and prepare model for sharing\n",
    "print(\"💾 SAVING AND PREPARING MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create model card with training information\n",
    "model_card_content = f\"\"\"\n",
    "# Gemma 1B Fine-tuned Model\n",
    "\n",
    "## Model Description\n",
    "This is a fine-tuned version of Google's Gemma 1B model, adapted for instruction-following tasks.\n",
    "\n",
    "## Training Details\n",
    "- **Base model**: {MODEL_NAME}\n",
    "- **Fine-tuning method**: LoRA (Low-Rank Adaptation)\n",
    "- **Training device**: {device}\n",
    "- **LoRA rank**: {LORA_R}\n",
    "- **LoRA alpha**: {LORA_ALPHA}\n",
    "- **Training epochs**: {training_args.num_train_epochs}\n",
    "- **Learning rate**: {training_args.learning_rate}\n",
    "- **Batch size**: {effective_batch_size} (effective)\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1b\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-1b\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/this/model\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"### Instruction:\\\\nYour question here\\\\n\\\\n### Response:\\\\n\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=200, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "## Training Data\n",
    "The model was fine-tuned on a custom instruction-following dataset.\n",
    "\n",
    "## Limitations\n",
    "- This is a demonstration model with limited training data\n",
    "- May not generalize well to all tasks\n",
    "- Requires the same format for optimal performance\n",
    "\n",
    "## License\n",
    "This model inherits the license from the base Gemma model.\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(f\"{output_dir}/README.md\", \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "\n",
    "print(\"📝 Model card created\")\n",
    "\n",
    "# Save training configuration\n",
    "training_config = {\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"device\": device,\n",
    "    \"lora_config\": {\n",
    "        \"r\": LORA_R,\n",
    "        \"alpha\": LORA_ALPHA,\n",
    "        \"dropout\": LORA_DROPOUT,\n",
    "        \"target_modules\": target_modules\n",
    "    },\n",
    "    \"training_args\": training_args.to_dict() if hasattr(training_args, 'to_dict') else str(training_args),\n",
    "    \"dataset_size\": len(extended_data),\n",
    "    \"max_length\": MAX_LENGTH\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2, default=str)\n",
    "\n",
    "print(\"⚙️ Training configuration saved\")\n",
    "\n",
    "# List all saved files\n",
    "print(f\"\\n📁 SAVED FILES IN {output_dir}:\")\n",
    "try:\n",
    "    for file in os.listdir(output_dir):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"   {file} ({size_mb:.1f} MB)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error listing files: {e}\")\n",
    "\n",
    "# Instructions for using the model\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(f\"1. Test the model thoroughly with your use case\")\n",
    "print(f\"2. If performance is good, consider training with more data\")\n",
    "print(f\"3. You can push to HuggingFace Hub for sharing:\")\n",
    "print(f\"   model.push_to_hub('your-username/gemma-1b-finetuned')\")\n",
    "print(f\"4. Or share the '{output_dir}' folder directly\")\n",
    "\n",
    "print(f\"\\n✅ Model preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully completed the fine-tuning process for Gemma 1B. Here's what you've accomplished:\n",
    "\n",
    "### ✅ What you achieved:\n",
    "1. **Environment Setup**: Configured the system for different devices (CPU, CUDA, Apple Silicon)\n",
    "2. **Model Loading**: Successfully loaded and prepared Gemma 1B for fine-tuning\n",
    "3. **Dataset Preparation**: Created and formatted training data for instruction-following\n",
    "4. **LoRA Implementation**: Applied efficient fine-tuning with Low-Rank Adaptation\n",
    "5. **Training Execution**: Ran the complete fine-tuning process\n",
    "6. **Model Evaluation**: Tested the fine-tuned model's performance\n",
    "7. **Model Deployment**: Saved and prepared the model for sharing\n",
    "\n",
    "### 🔑 Key concepts learned:\n",
    "- **Parameter-Efficient Fine-Tuning**: Using LoRA to reduce computational requirements\n",
    "- **Device Optimization**: Configuring training for different hardware\n",
    "- **Dataset Formatting**: Preparing instruction-following datasets\n",
    "- **Training Monitoring**: Understanding metrics and performance\n",
    "- **Model Evaluation**: Testing and validating fine-tuned models\n",
    "\n",
    "### 🚀 Improvement strategies:\n",
    "\n",
    "#### For better results:\n",
    "1. **More Training Data**: Use 1000+ high-quality examples\n",
    "2. **Longer Training**: Increase epochs and fine-tune learning rate\n",
    "3. **Better Data Quality**: Clean, diverse, and relevant examples\n",
    "4. **Hyperparameter Tuning**: Experiment with LoRA rank, learning rate, batch size\n",
    "5. **Evaluation Metrics**: Implement proper evaluation beyond loss\n",
    "\n",
    "#### Advanced techniques:\n",
    "1. **QLoRA**: Quantized LoRA for even more efficiency\n",
    "2. **Multi-task Training**: Train on multiple tasks simultaneously\n",
    "3. **Reinforcement Learning from Human Feedback (RLHF)**: Align with human preferences\n",
    "4. **Curriculum Learning**: Progressive training difficulty\n",
    "5. **Model Merging**: Combine multiple fine-tuned adapters\n",
    "\n",
    "### 💡 Production considerations:\n",
    "- **Quantization**: Use 8-bit or 4-bit quantization for deployment\n",
    "- **Optimization**: ONNX conversion or TensorRT for inference speed\n",
    "- **Monitoring**: Track model performance in production\n",
    "- **Safety**: Implement content filtering and bias detection\n",
    "- **Versioning**: Keep track of model versions and training data\n",
    "\n",
    "### 🛠️ Troubleshooting tips:\n",
    "- **Memory Issues**: Reduce batch size, use gradient checkpointing, or try CPU training\n",
    "- **Slow Training**: Check device utilization, use mixed precision, optimize data loading\n",
    "- **Poor Performance**: Increase training data, adjust learning rate, check data quality\n",
    "- **Overfitting**: Use validation split, early stopping, or regularization\n",
    "\n",
    "### 📚 Further learning:\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [Gemma Model Documentation](https://huggingface.co/docs/transformers/model_doc/gemma)\n",
    "- [Fine-tuning Best Practices](https://huggingface.co/blog/rlhf)\n",
    "\n",
    "### 🎉 Congratulations!\n",
    "You now have a working fine-tuned Gemma 1B model and the knowledge to improve it further. The techniques you've learned can be applied to other models and tasks. Happy fine-tuning! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
